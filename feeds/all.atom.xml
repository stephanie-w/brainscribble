<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Brain Scribble</title><link href="http://stephanie-w.github.io/blog/" rel="alternate"></link><link href="http://stephanie-w.github.io/blog/feeds/all.atom.xml" rel="self"></link><id>http://stephanie-w.github.io/blog/</id><updated>2015-08-21T00:00:00+02:00</updated><entry><title>Machine Learning Links</title><link href="http://stephanie-w.github.io/blog/machine-learning-links.html" rel="alternate"></link><updated>2015-08-21T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-08-21:blog/machine-learning-links.html</id><summary type="html">&lt;hr /&gt;
&lt;p&gt;&lt;a href="http://www.cbcb.umd.edu/~hcorrada/PracticalML/"&gt;Practical machine learning: methods and algorithmics&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://www.iro.umontreal.ca/~bengioy/dlbook/"&gt;Deep Learning&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Caret (R Library for Machine Learning):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tutorials:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.edii.uclm.es/%7EuseR-2013/Tutorials/kuhn/user_caret_2up.pdf"&gt;http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf&lt;/li&gt;
&lt;li&gt;&lt;a href="http://topepo.github.io/caret/training.html"&gt;Model Training and Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://topepo.github.io/caret/visualizations.html"&gt;Visualizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://topepo.github.io/caret/preprocess.html"&gt;Pre-Processing&lt;/a&gt;
Papers:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jstatsoft.org/v28/i05/paper"&gt;Building Predictive Models in R Using the caret Package&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.dataschool.io/comparing-supervised-learning-algorithms/"&gt;Comparing Supervised Learning Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stat.ethz.ch/education/semesters/FS_2008/CompStat/sk-ch8.pdf"&gt;Bagging and Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm"&gt;Random Forests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Boosting:&lt;ul&gt;
&lt;li&gt;Tutorials:&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf"&gt;A Tutorial on Boosting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf"&gt;Boosting Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Papers:&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf"&gt;The BigChaos Solution to the Netflix Grand Prize&lt;/a&gt;
$ &lt;a href="https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf"&gt;My milestone 1 solution to the Heritage Health Prize&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.stat.washington.edu/raftery/Research/PDF/fraley2002.pdf"&gt;Model-based clustering, discriminant analysis, and density estimation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.otexts.org/fpp/"&gt;Forecasting: principles and practice&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Nim Games</title><link href="http://stephanie-w.github.io/blog/nim-games.html" rel="alternate"></link><updated>2015-08-15T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-08-15:blog/nim-games.html</id><summary type="html">
&lt;hr/&gt;
&lt;!-- BEGIN_SUMMARY --&gt;
&lt;p&gt;Nim is a mathematical game of strategy in which two players take turns removing objects from distinct heaps. On each turn, a player must remove at least one object, and may remove any number of objects provided they all come from the same heap.&lt;br/&gt;
Nim can be played as a misère game, in which the player to take the last object loses or as a normal play game, in wich the person who makes the last move (i.e., who takes the last object) wins.&lt;/p&gt;
&lt;!-- END_SUMMARY --&gt;
&lt;p&gt;We'll explore the normal play game here.&lt;/p&gt;
&lt;p&gt;For example, on a table, there is heaps of stones (or matches or coins) from various sizes. Each player takes one or several stones in a single heap. The winner is the one who can empty the table.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/nim-game-ex.png"/&gt;&lt;/p&gt;
&lt;h2 id="game-graph"&gt;Game Graph&lt;/h2&gt;
&lt;p&gt;The following graph represents a game with three heaps containing 1, 2 and 3 matches respectively.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/nim-graph.png"/&gt;&lt;/p&gt;
&lt;p&gt;The graph shows the allowed and possible state after a given state.&lt;br/&gt;
For example from (1,1,2), the game rule allows the following states (1,1,1) or (1,2) or (1,1).&lt;/p&gt;
&lt;p&gt;The nodes in red compose the graph core. The graph core has the following property:&lt;/p&gt;
&lt;p&gt;(1) Every node out of a core has at least one successor, and among them a node of the core&lt;br/&gt;
(2) Any node from the core can't have as successor (if exist) a node from the core  &lt;/p&gt;
&lt;p&gt;A game state or position (or option) can be inside or outside the core. The loosing position(s) is inside the core (from first property).  &lt;/p&gt;
&lt;p&gt;The first position can be inside or outside the core.&lt;br/&gt;
Let's say it is outside, there is a winning strategy for the first player: step in the core (possible, cause of the first property), the second player will step out from the core (from the second property). Then the first player will step in the core once again, etc. If the first position was inside the core, then the second player could adopt the wining strategy of step in, step out the core alternatively.&lt;/p&gt;
&lt;p&gt;In the context of a finding a winning strategy, properties above can be rewritten as follow:&lt;/p&gt;
&lt;p&gt;(1') From a winning position, there is at least one move to a losing position (in the core)&lt;br/&gt;
(2') From a losing position, all moves lead to a winning position (outside the core)  &lt;/p&gt;
&lt;h2 id="determining-the-graph-core"&gt;Determining the graph core&lt;/h2&gt;
&lt;h3 id="the-xor-way"&gt;The XOR way&lt;/h3&gt;
&lt;p&gt;Let's try to find a way to compute a positive number for each node determining if the node is in the core or not.&lt;br/&gt;
Let's 0, the number assigned to the nodes of the core, so any number different from 0 is outside the core.  &lt;/p&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(x_1,\)&lt;/span&gt; ..., &lt;span class="math"&gt;\(x_n\)&lt;/span&gt; be the sizes of the heaps before a move, and &lt;span class="math"&gt;\(y_1\)&lt;/span&gt;, ..., &lt;span class="math"&gt;\(y_n\)&lt;/span&gt; the corresponding sizes after a move. 
We have to find an operation ? where, for a given position:&lt;br/&gt;
(1') if s = &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; ? &lt;span class="math"&gt;\(x_1 ? ... ? x_n = 0\)&lt;/span&gt; then all successor positions of this position verifing t = &lt;span class="math"&gt;\(y_0\)&lt;/span&gt; ? &lt;span class="math"&gt;\(y_1 ? ... ? y_n \ne 0\)&lt;/span&gt;&lt;br/&gt;
(1') s = if &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; ? &lt;span class="math"&gt;\(x_1 ? ... ? x_n \ne 0\)&lt;/span&gt; it exists a least a successor position of this position verifing t = &lt;span class="math"&gt;\(y_0\)&lt;/span&gt; ? &lt;span class="math"&gt;\(y_1 ? ... ? y_n = 0\)&lt;/span&gt;&lt;br/&gt;
with &lt;span class="math"&gt;\(x_i = y_i\)&lt;/span&gt; for all &lt;span class="math"&gt;\(i \ne k\)&lt;/span&gt;, and &lt;span class="math"&gt;\(x_k &amp;gt; y_k\)&lt;/span&gt; (any move from a position to an other can only change one of the &lt;span class="math"&gt;\(x_k\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;A such operation is the XOR (exclusive or) operation sybolized by &lt;span class="math"&gt;\(\oplus\)&lt;/span&gt;.&lt;br/&gt;
XOR is a logical bitwise operation, XOR is true only when an odd number of inputs is true.&lt;/p&gt;
&lt;p&gt;XOR Thruth Table:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;p&lt;/th&gt;
&lt;th&gt;q&lt;/th&gt;
&lt;th&gt;&lt;span class="math"&gt;\(p \oplus q\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;0 = FALSE&lt;/em&gt;&lt;br/&gt;
&lt;em&gt;1 = TRUE&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Proof:&lt;br/&gt;
(1') : If you change a single bits in a XOR of bits, you'll change the final result, ie. if &lt;span class="math"&gt;\(s = 0\)&lt;/span&gt; then &lt;span class="math"&gt;\(t \ne 0\)&lt;/span&gt; &lt;br/&gt;
(2') : From s, there is at least one way to get a sum t equal to 0 while respecting the rule of the game (ie. removing elements from a single heap): Let d be the position of the  leftmost one bit in s and k a heap where the dth bit is also one (it has to exist), then from position d (included) flip all the bit of &lt;span class="math"&gt;\(x_k\)&lt;/span&gt; correponding to the 1 in the sum s. The new sum s will be 0.&lt;/p&gt;
&lt;p&gt;Example : The sum is 01011 (the leftmost nonzero bit is at 2th bit). Using the first heap, flipping the 2nd, 4th and 5th bits:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;    *&lt;/span&gt;
&lt;span class="code-line"&gt;  1 1 0 0 1         1 0 0 1 0 &lt;/span&gt;
&lt;span class="code-line"&gt;  0 1 1 1 1         0 1 1 1 1&lt;/span&gt;
&lt;span class="code-line"&gt;  1 0 0 1 1         1 0 0 1 1&lt;/span&gt;
&lt;span class="code-line"&gt;  0 0 0 1 1    =&amp;gt;   0 0 0 1 1&lt;/span&gt;
&lt;span class="code-line"&gt;  0 1 1 0 1         0 1 1 0 1&lt;/span&gt;
&lt;span class="code-line"&gt;  ---------         ---------&lt;/span&gt;
&lt;span class="code-line"&gt;  0 1 0 1 1         0 0 0 0 0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We verify  &lt;span class="math"&gt;\(y_k\)&lt;/span&gt; &amp;lt; &lt;span class="math"&gt;\(x_k\)&lt;/span&gt; : all bits to the left of d are the same in &lt;span class="math"&gt;\(x_k\)&lt;/span&gt; and &lt;span class="math"&gt;\(y_k\)&lt;/span&gt;, bit d decreases from 1 to 0 (decreasing the value by 2d), and any change in the remaining bits will amount to at most &lt;span class="math"&gt;\(2^d-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;By removing &lt;span class="math"&gt;\(x_k\)&lt;/span&gt; - &lt;span class="math"&gt;\(y_k\)&lt;/span&gt; objects from heap k, we get a let a loosing position for the other player.&lt;/p&gt;
&lt;p&gt;From our previous graph, applying XOR sum on heap sizes, we get: &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/nim-graph-2.png"/&gt;&lt;/p&gt;
&lt;p&gt;By computing these numbers to the graph, we define a Grundy function. The Grundy numbers are also called nimbers, as value of Nim heap.&lt;/p&gt;
&lt;p&gt;For any finite combinatorial game, there is only one way to decompose the (finite) set of positions into nimbers.&lt;/p&gt;
&lt;h3 id="the-hard-way"&gt;The hard way&lt;/h3&gt;
&lt;p&gt;An other way to find the core:&lt;/p&gt;
&lt;p&gt;A number can be assigned to each node so it follows the following property: The number of each node is the smallest number not in the successor numbers list.&lt;br/&gt;
For example A with number 3 has for successors B, C, D, F, then the successor numbers list is (0, 1, 2).&lt;/p&gt;
&lt;p&gt;This is an illustration of the Sprague-Grundy theorem which states that every impartial game is equivalent to a nim heap of a certain size.
Nimber addition (also known as nim-addition) can be used to calculate the size of a single heap equivalent to a collection of heaps. It is defined recursively by:&lt;/p&gt;
&lt;div class="math"&gt;$$\alpha \oplus \beta = \operatorname{mex}(\{\,\alpha' \oplus \beta : \alpha' &amp;lt; \alpha\,\} \cup \{\, \alpha  \oplus \beta' : \beta' &amp;lt; \beta \,\})$$&lt;/div&gt;
&lt;p&gt;where for a set S of ordinals, mex(S) is defined to be the "minimum excluded ordinal", i.e. mex(S) is the smallest ordinal which is not an element of S.&lt;/p&gt;
&lt;p&gt;From our previous properties:&lt;br/&gt;
(1) Every node out of a core has at least one successor, and among them a node of the core&lt;br/&gt;
(2) Any node from the core can't have as successor (if exist) a node from the core  &lt;/p&gt;
&lt;p&gt;the relation with the Grundy number 0 and the core is a follow:&lt;br/&gt;
A number outside the core has a Grundy Number &amp;gt; 0, then 0 is in its successor number list (in respect of the first(1)).
A node with number 0 has as successors only successors with numbers not equal to 0, and then outside the core (in respect of (2)).&lt;/p&gt;
&lt;p&gt;For any finite combinatorial game, there is only one way to decompose the (finite) set of positions into two classes which satisfy (1) and (2).&lt;/p&gt;
&lt;p&gt;Note :&lt;br/&gt;
Example of a digital sum &lt;span class="math"&gt;\(7 \oplus 11\)&lt;/span&gt;: &lt;br/&gt;
&lt;span class="math"&gt;\(7 = 4 + 2 + 1 = 1.2^{2} + 1.2^{1} + 1.2^{0} = 111\)&lt;/span&gt;&lt;br/&gt;
&lt;span class="math"&gt;\(11 = 8 + 2 + 1 = 1.2^{3} + 0.2^{2} + 1.2^{1} + 1.2^{0} = 1011\)&lt;/span&gt;&lt;br/&gt;
Then we sum, the base 2 numbers, with 1 + 1 = 0  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;    1 1 1&lt;/span&gt;
&lt;span class="code-line"&gt;+ 1 0 1 1&lt;/span&gt;
&lt;span class="code-line"&gt; --------&lt;/span&gt;
&lt;span class="code-line"&gt;  1 1 0 0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(1100= 1.2^{3} + 1.2^{2} + 0.2^{1} + 0.2^{0} = 12\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(7 \oplus 11 = 12\)&lt;/span&gt;&lt;/p&gt;
&lt;!--
Sources:
https://en.wikipedia.org/wiki/Nim
https://en.wikipedia.org/wiki/Nimber
https://en.wikipedia.org/wiki/Exclusive_or
http://math.stackexchange.com/questions/416042/why-xor-operator-works
--&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary></entry><entry><title>Confusion Matrix</title><link href="http://stephanie-w.github.io/blog/confusion-matrix.html" rel="alternate"></link><updated>2015-08-12T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-08-12:blog/confusion-matrix.html</id><summary type="html">&lt;hr /&gt;
&lt;!-- BEGIN_SUMMARY --&gt;

&lt;p&gt;A confusion matrix, also known as a contingency table or an error matrix or tavle of confusion, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix).&lt;/p&gt;
&lt;!-- END_SUMMARY --&gt;

&lt;p&gt;It is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives.&lt;/p&gt;
&lt;p&gt;For example, for a test that screens people for a given disease, the confusion matrix will be:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/confusion-matrix-0.png" /&gt;&lt;/p&gt;
&lt;p&gt;with&lt;br /&gt;
x true positives (TP) : the number of sick people correctly identified as sick&lt;br /&gt;
z false positives (FP) : the number of healthy people incorrectly identified as sick&lt;br /&gt;
t true negatives (TN) : the number of healthy people correctly identified as healthy&lt;br /&gt;
y false negatives (FN) : the number of sick people incorrectly identified as healthy  &lt;/p&gt;
&lt;p&gt;The following probabilities are associated with the confusion matrix:  &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Sensitivity = Pr\left(positive\ test\ |\ disease\right)\)&lt;/span&gt;&lt;br /&gt;
&lt;span class="math"&gt;\(Specificity = Pr\left(negative\ test\ |\ no\ disease\right)\)&lt;/span&gt;&lt;br /&gt;
&lt;span class="math"&gt;\(Positive\ Predictive\ Value = Pr\left(disease\ |\ positive\ test\right)\)&lt;/span&gt;&lt;br /&gt;
&lt;span class="math"&gt;\(Negative\ Predictive\ Value =  Pr\left(no disease\ |\ negative\ test\right)\)&lt;/span&gt;&lt;br /&gt;
&lt;span class="math"&gt;\(Accuracy = Pr\left(correct\ outcome\right)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which are computed the following way:&lt;/p&gt;
&lt;div class="math"&gt;$$Sensitivity = \frac{TP}{TP+FN}$$&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$Specificity = \frac{TN}{FP+TN}$$&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$Positive\ Predictive\ Value = \frac{TP}{TP+FP}$$&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$Negative\ Predictive\ Value = \frac{TN}{FN+TN}$$&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$Accuracy = \frac{TP+TN}{TP+FP+FN+TN}$$&lt;/div&gt;
&lt;h2 id="examples"&gt;Examples&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Example 1&lt;/em&gt; 
A diagnostic test with sensitivity 67% and specificity 91% is applied to 2030 people to look for a disorder with a population prevalence of 1.48%.&lt;/p&gt;
&lt;p&gt;Let's build the associated 2×2 contingency table:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/confusion-matrix-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/confusion-matrix-2.png" /&gt;&lt;/p&gt;
&lt;p&gt;To summary and more simply:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There were 2030 people tested, then 2030 predictions&lt;/li&gt;
&lt;li&gt;Out of those 2030 tests, 200 were identified as sick, 1830 as healthy&lt;/li&gt;
&lt;li&gt;In reality, 30 people are sick, 2000 are healthy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Example 2&lt;/em&gt; 
Suppose that we have created a machine learning algorithm that predicts whether a link will be clicked with 99% sensitivity and 99% specificity. The rate the link is clicked is 1/1000 of visits to a website. If we predict the link will be clicked on a specific visit, what is the probability it will actually be clicked?&lt;/p&gt;
&lt;p&gt;Let's be 100000 the number of visits:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/confusion-matrix-3.png" /&gt;&lt;/p&gt;
&lt;p&gt;According to the confusion matrix above, the probability that the link will be actually clicked is 9%.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary></entry><entry><title>R : Linear Model Diagnosis for Machine Learning Modeling</title><link href="http://stephanie-w.github.io/blog/linear-model-diagnosis-for-machine-learning.html" rel="alternate"></link><updated>2015-08-10T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-08-10:blog/linear-model-diagnosis-for-machine-learning.html</id><summary type="html">
&lt;hr/&gt;
&lt;!-- BEGIN_SUMMARY --&gt;
&lt;p&gt;This post follows the &lt;a href=""&gt;R : Exploring Data for Machine Learning Modeling&lt;/a&gt; post, exploring linear model in the context of machine learning.&lt;/p&gt;
&lt;!-- END_SUMMARY --&gt;
&lt;p&gt;We'll still use the Wage dataset from the ISLR package.&lt;br/&gt;
This dataset reports wage and other data (age, education, jobclass, etc.) for a group of 3000 male workers in the Mid-Atlantic region.&lt;/p&gt;
&lt;h2 id="linear-model"&gt;Linear Model&lt;/h2&gt;
&lt;p&gt;We build a training and testing set (50% of the Wage dataset each):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;ISLR&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="p"&gt;(&lt;/span&gt;Wage&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;caret&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;intrain &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; createDataPartition&lt;span class="p"&gt;(&lt;/span&gt;y &lt;span class="o"&gt;=&lt;/span&gt; Wage&lt;span class="o"&gt;$&lt;/span&gt;wage&lt;span class="p"&gt;,&lt;/span&gt; p &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;training &lt;span class="o"&gt;=&lt;/span&gt; Wage&lt;span class="p"&gt;[&lt;/span&gt;intrain&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;testing &lt;span class="o"&gt;=&lt;/span&gt; Wage&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;intrain&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We'll fit a linear model on the training set with wage as outcome and age, jobclass and education as predictors:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;fit &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; train&lt;span class="p"&gt;(&lt;/span&gt;wage &lt;span class="o"&gt;~&lt;/span&gt; age &lt;span class="o"&gt;+&lt;/span&gt; jobclass &lt;span class="o"&gt;+&lt;/span&gt; education&lt;span class="p"&gt;,&lt;/span&gt; method &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"lm"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; training&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;final &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; fit&lt;span class="o"&gt;$&lt;/span&gt;finalModel&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;fit&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## Linear Regression &lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## 1501 samples&lt;/span&gt;
&lt;span class="code-line"&gt;##   11 predictor&lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## No pre-processing&lt;/span&gt;
&lt;span class="code-line"&gt;## Resampling: Bootstrapped (25 reps) &lt;/span&gt;
&lt;span class="code-line"&gt;## Summary of sample sizes: 1501, 1501, 1501, 1501, 1501, 1501, ... &lt;/span&gt;
&lt;span class="code-line"&gt;## Resampling results&lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;##   RMSE      Rsquared   RMSE SD  Rsquared SD&lt;/span&gt;
&lt;span class="code-line"&gt;##   34.28111  0.2566582  1.43661  0.03048721 &lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="diagnosis-plots"&gt;Diagnosis plots&lt;/h2&gt;
&lt;h3 id="residuals-vs-fitted"&gt;Residuals vs fitted&lt;/h3&gt;
&lt;p&gt;Plotting residuals by the predictions (the fitted values) in the training set against the residuals, ie. amont of variation left over after the model fit. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;plot&lt;span class="p"&gt;(&lt;/span&gt;final&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; pch &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;19&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; cex &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; col &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"#00000010"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="rimage center"&gt;&lt;img src="figure/linear-model-diagnosis-for-machine-learning-3-1.png"/&gt;&lt;/div&gt;
&lt;p&gt;The red line should be centered at 0 on the y-axis (since it represents the difference between the real values and the fitted values).&lt;br/&gt;
Outliners labeled at the top of the graph should be explored further for identifying explanotary variables. &lt;/p&gt;
&lt;p&gt;We can then using colors for coloring a variable not used in the model to help spotting a trend in that variable.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;qplot&lt;span class="p"&gt;(&lt;/span&gt;final&lt;span class="o"&gt;$&lt;/span&gt;fitted&lt;span class="p"&gt;,&lt;/span&gt; final&lt;span class="o"&gt;$&lt;/span&gt;residuals&lt;span class="p"&gt;,&lt;/span&gt; color &lt;span class="o"&gt;=&lt;/span&gt; race&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; training&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="rimage center"&gt;&lt;img alt="plot of chunk unnamed-chunk-4" class="plot" src="figure/linear-model-diagnosis-for-machine-learning-4-1.png" title="plot of chunk unnamed-chunk-4"/&gt;&lt;/div&gt;
&lt;p&gt;We see in the graph above that some of the outliners might be explained by the race variable.&lt;/p&gt;
&lt;h3 id="plots-by-index"&gt;Plots by index&lt;/h3&gt;
&lt;p&gt;Plotting residuals vs index (ie; row numbers) can be helpful in showing missing variables.&lt;br/&gt;
Residuals should not have relationship to index (ie. order of observations).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;plot&lt;span class="p"&gt;(&lt;/span&gt;final&lt;span class="o"&gt;$&lt;/span&gt;residuals&lt;span class="p"&gt;,&lt;/span&gt; pch &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;19&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; cex &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="rimage center"&gt;&lt;img src="figure/linear-model-diagnosis-for-machine-learning-5-1.png"/&gt;&lt;/div&gt;
&lt;p&gt;We're looking for residuals appearing at higher or lower rows and/or trends with respect to row numbers, suggesting there is a variable missing from the model, some continuous variable that the rows are ordered by (tipycally a variable expressing a relationship with respect to time or age).&lt;/p&gt;
&lt;h3 id="predicted-vs-real-values-plot"&gt;Predicted vs real values plot&lt;/h3&gt;
&lt;p&gt;Plotting wage variable in the test set vs the predicted values in the test set.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;pred &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; predict&lt;span class="p"&gt;(&lt;/span&gt;fit&lt;span class="p"&gt;,&lt;/span&gt; testing&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;qplot&lt;span class="p"&gt;(&lt;/span&gt;wage&lt;span class="p"&gt;,&lt;/span&gt; pred&lt;span class="p"&gt;,&lt;/span&gt; color &lt;span class="o"&gt;=&lt;/span&gt; year&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; testing&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="rimage center"&gt;&lt;img src="figure/linear-model-diagnosis-for-machine-learning-6-1.png"/&gt;&lt;/div&gt;
&lt;p&gt;Ideally, these varaible are close to each other and there is a straight line on the 45° line.&lt;/p&gt;</summary><category term="R"></category></entry><entry><title>R : Exploring Data for Machine Learning Modeling</title><link href="http://stephanie-w.github.io/blog/exploring-data-for-marchine-learning.html" rel="alternate"></link><updated>2015-08-09T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-08-09:blog/exploring-data-for-marchine-learning.html</id><summary type="html">
&lt;hr/&gt;
&lt;!-- BEGIN_SUMMARY --&gt;
&lt;p&gt;These are my notes on the Practical Machine Learning course (Week2: Plotting Predictors - Tutorial).&lt;/p&gt;
&lt;!-- END_SUMMARY --&gt;
&lt;p&gt;When exploring data for Machine Learning, we're looking for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;imbalance outcomes/predictors&lt;/li&gt;
&lt;li&gt;outliners&lt;/li&gt;
&lt;li&gt;groups of outcome points not explained by any of the predictors&lt;/li&gt;
&lt;li&gt;skewed variables (that needs to be transformed)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We'll use the Wage dataset from the ISLR package.&lt;br/&gt;
This dataset reports wage and other data (age, education, jobclass, etc.) for a group of 3000 male workers in the Mid-Atlantic region.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;ISLR&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="p"&gt;(&lt;/span&gt;Wage&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;Wage&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##       year           age               sex                    maritl           race     &lt;/span&gt;
&lt;span class="code-line"&gt;##  Min.   :2003   Min.   :18.00   1. Male  :3000   1. Never Married: 648   1. White:2480  &lt;/span&gt;
&lt;span class="code-line"&gt;##  1st Qu.:2004   1st Qu.:33.75   2. Female:   0   2. Married      :2074   2. Black: 293  &lt;/span&gt;
&lt;span class="code-line"&gt;##  Median :2006   Median :42.00                    3. Widowed      :  19   3. Asian: 190  &lt;/span&gt;
&lt;span class="code-line"&gt;##  Mean   :2006   Mean   :42.41                    4. Divorced     : 204   4. Other:  37  &lt;/span&gt;
&lt;span class="code-line"&gt;##  3rd Qu.:2008   3rd Qu.:51.00                    5. Separated    :  55                  &lt;/span&gt;
&lt;span class="code-line"&gt;##  Max.   :2009   Max.   :80.00                                                           &lt;/span&gt;
&lt;span class="code-line"&gt;##                                                                                         &lt;/span&gt;
&lt;span class="code-line"&gt;##               education                     region               jobclass               health    &lt;/span&gt;
&lt;span class="code-line"&gt;##  1. &amp;lt; HS Grad      :268   2. Middle Atlantic   :3000   1. Industrial :1544   1. &amp;lt;=Good     : 858  &lt;/span&gt;
&lt;span class="code-line"&gt;##  2. HS Grad        :971   1. New England       :   0   2. Information:1456   2. &amp;gt;=Very Good:2142  &lt;/span&gt;
&lt;span class="code-line"&gt;##  3. Some College   :650   3. East North Central:   0                                              &lt;/span&gt;
&lt;span class="code-line"&gt;##  4. College Grad   :685   4. West North Central:   0                                              &lt;/span&gt;
&lt;span class="code-line"&gt;##  5. Advanced Degree:426   5. South Atlantic    :   0                                              &lt;/span&gt;
&lt;span class="code-line"&gt;##                           6. East South Central:   0                                              &lt;/span&gt;
&lt;span class="code-line"&gt;##                           (Other)              :   0                                              &lt;/span&gt;
&lt;span class="code-line"&gt;##   health_ins      logwage           wage       &lt;/span&gt;
&lt;span class="code-line"&gt;##  1. Yes:2083   Min.   :3.000   Min.   : 20.09  &lt;/span&gt;
&lt;span class="code-line"&gt;##  2. No : 917   1st Qu.:4.447   1st Qu.: 85.38  &lt;/span&gt;
&lt;span class="code-line"&gt;##                Median :4.653   Median :104.92  &lt;/span&gt;
&lt;span class="code-line"&gt;##                Mean   :4.654   Mean   :111.70  &lt;/span&gt;
&lt;span class="code-line"&gt;##                3rd Qu.:4.857   3rd Qu.:128.68  &lt;/span&gt;
&lt;span class="code-line"&gt;##                Max.   :5.763   Max.   :318.34  &lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Building training and testing sets (50% of the Wage dataset each):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;caret&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;intrain &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; createDataPartition&lt;span class="p"&gt;(&lt;/span&gt;y &lt;span class="o"&gt;=&lt;/span&gt; Wage&lt;span class="o"&gt;$&lt;/span&gt;wage&lt;span class="p"&gt;,&lt;/span&gt; p &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;training &lt;span class="o"&gt;=&lt;/span&gt; Wage&lt;span class="p"&gt;[&lt;/span&gt;intrain&lt;span class="p"&gt;,]&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;testing &lt;span class="o"&gt;=&lt;/span&gt; Wage&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;intrain&lt;span class="p"&gt;,]&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The exploration is always done on the training set.&lt;/p&gt;
&lt;h2 id="plotting-predictors-against-outcome"&gt;Plotting predictors against outcome&lt;/h2&gt;
&lt;p&gt;Plotting wage versus age, education and jobclass using the R featurePlot function (from the caret package):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;featurePlot&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; training&lt;span class="p"&gt;[,&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"age"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"education"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"jobclass"&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt; y &lt;span class="o"&gt;=&lt;/span&gt; training&lt;span class="o"&gt;$&lt;/span&gt;wage&lt;span class="p"&gt;,&lt;/span&gt; plot &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"pairs"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="rimage center"&gt;&lt;img src="figure/exploring-data-for-marchine-learning-3-1.png"/&gt;&lt;/div&gt;
&lt;p&gt;Plotting wage versus age:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;ggplot2&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;qplot&lt;span class="p"&gt;(&lt;/span&gt;age&lt;span class="p"&gt;,&lt;/span&gt; wage&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; training&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="rimage center"&gt;&lt;img src="figure/exploring-data-for-marchine-learning-4-2.png"/&gt;&lt;/div&gt;
&lt;p&gt;The graph shows some patterns: a trend in wages comparing to ages and a group of outlined observations (above 250 dollars raw wage).&lt;/p&gt;
&lt;p&gt;Plotting wage versus age, grouping by jobclass:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;ggplot2&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="rimage center"&gt;&lt;img src="figure/exploring-data-for-marchine-learning-4-1.png"/&gt;&lt;/div&gt;
&lt;p&gt;The jobclass difference could explain the two distinct groups.&lt;br/&gt;
The jobclass variable might be able to predict at least a part of the variability that appears in the top of the plot.&lt;/p&gt;
&lt;p&gt;Plotting wage versus age, grouping by education, adding regression smoothers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;qq &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; qplot&lt;span class="p"&gt;(&lt;/span&gt;age&lt;span class="p"&gt;,&lt;/span&gt; wage&lt;span class="p"&gt;,&lt;/span&gt; color &lt;span class="o"&gt;=&lt;/span&gt; education&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; training&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;qq &lt;span class="o"&gt;+&lt;/span&gt; geom_smooth&lt;span class="p"&gt;(&lt;/span&gt;method &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"lm"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; formula &lt;span class="o"&gt;=&lt;/span&gt; y &lt;span class="o"&gt;~&lt;/span&gt; x&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="rimage center"&gt;&lt;img src="figure/exploring-data-for-marchine-learning-5-1.png"/&gt;&lt;/div&gt;
&lt;p&gt;The "Advanced Degree" education seems to also explained a lot of the variation at the top.&lt;/p&gt;
&lt;h2 id="data-repartition"&gt;Data Repartition&lt;/h2&gt;
&lt;p&gt;Breaking up the wage variable into three groups (factors actually) with the R cut2 function (from the Hmisc package):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;Hmisc&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;cutWage &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; cut2&lt;span class="p"&gt;(&lt;/span&gt;training&lt;span class="o"&gt;$&lt;/span&gt;wage&lt;span class="p"&gt;,&lt;/span&gt; g &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;cutWage&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## cutWage&lt;/span&gt;
&lt;span class="code-line"&gt;## [ 20.1, 91.7) [ 91.7,118.9) [118.9,314.3] &lt;/span&gt;
&lt;span class="code-line"&gt;##           506           519           476&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Looking at the repartition, we can see that there are more industrial jobs that there are information jobs with lower wage. Then the trend reverses itself.&lt;/p&gt;
&lt;p&gt;Plotting a boxplot of the wage groups created above:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;qplot&lt;span class="p"&gt;(&lt;/span&gt;cutWage&lt;span class="p"&gt;,&lt;/span&gt; age&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; training&lt;span class="p"&gt;,&lt;/span&gt; fill &lt;span class="o"&gt;=&lt;/span&gt; cutWage&lt;span class="p"&gt;,&lt;/span&gt; geom &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"boxplot"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="rimage center"&gt;&lt;img src="figure/exploring-data-for-marchine-learning-7-1.png"/&gt;&lt;/div&gt;
&lt;p&gt;Exploring the repartition of jobclass across wage groups:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;t1 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;cutWage&lt;span class="p"&gt;,&lt;/span&gt; training&lt;span class="o"&gt;$&lt;/span&gt;jobclass&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;t1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##                &lt;/span&gt;
&lt;span class="code-line"&gt;## cutWage         1. Industrial 2. Information&lt;/span&gt;
&lt;span class="code-line"&gt;##   [ 20.1, 91.7)           313            193&lt;/span&gt;
&lt;span class="code-line"&gt;##   [ 91.7,118.9)           262            257&lt;/span&gt;
&lt;span class="code-line"&gt;##   [118.9,314.3]           190            286&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the prop function to get the proportion of jobclass (in each row) for each groups:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kp"&gt;prop.table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;t1&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##                &lt;/span&gt;
&lt;span class="code-line"&gt;## cutWage         1. Industrial 2. Information&lt;/span&gt;
&lt;span class="code-line"&gt;##   [ 20.1, 91.7)     0.6185771      0.3814229&lt;/span&gt;
&lt;span class="code-line"&gt;##   [ 91.7,118.9)     0.5048170      0.4951830&lt;/span&gt;
&lt;span class="code-line"&gt;##   [118.9,314.3]     0.3991597      0.6008403&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;62% of the low wage job correponds to industrial jobs, 38% to information jobs.&lt;/p&gt;
&lt;h2 id="density-plots"&gt;Density Plots&lt;/h2&gt;
&lt;p&gt;Density plot can be a much more effective way to view the distribution of a variable than boxplots.&lt;/p&gt;
&lt;p&gt;Plotting a density plot of the values of wages, grouping by education:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;qplot&lt;span class="p"&gt;(&lt;/span&gt;wage&lt;span class="p"&gt;,&lt;/span&gt; color &lt;span class="o"&gt;=&lt;/span&gt; education&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; training&lt;span class="p"&gt;,&lt;/span&gt; geom &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"density"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="rimage center"&gt;&lt;img src="figure/exploring-data-for-marchine-learning-10-1.png"/&gt;&lt;/div&gt;
&lt;p&gt;The "&amp;lt;HS grad" workers tend to have more values in the lower part of the range of wage. There is an outgroup of Advanced Degree and College Grad workers with higher wage.&lt;/p&gt;
&lt;p&gt;In the &lt;a href="linear-model-diagnosis-for-machine-learning.html"&gt;next post&lt;/a&gt;, we'll fit a linear model with wage as outcome and age, jobclass and education as predictors and perform some diagnosis analysis.&lt;/p&gt;</summary><category term="R"></category></entry><entry><title>Notes on Statistical Inference : Hypothesis Testing and t-tests</title><link href="http://stephanie-w.github.io/blog/hypothesis-testing-and-t-tests.html" rel="alternate"></link><updated>2015-07-23T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-07-23:blog/hypothesis-testing-and-t-tests.html</id><summary type="html">
&lt;hr/&gt;
&lt;p&gt;&lt;!-- BEGIN_SUMMARY --&gt;
These are  my notes on the Statistical Inference course (2th part) : Hypothesis Testing and t-tests&lt;/p&gt;
&lt;!-- END_SUMMARY --&gt;
&lt;h2 id="clt-central-limit-theorem"&gt;CLT : Central Limit Theorem&lt;/h2&gt;
&lt;p&gt;The distribution of sample statistics (e.g. mean) is approximatively normal, regardless of the underlying distribution, with mean = &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and variance = &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\bar{X} \sim N(mean = \mu, sd = \frac{\sigma}{\sqrt{n}})\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;Conditions for CLT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Independence : The sampled observations must be independent &lt;ul&gt;
&lt;li&gt;random sample / assignment&lt;/li&gt;
&lt;li&gt;if sampling without replacement, n &amp;lt; 10 % of population&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sample Size / skew :&lt;ul&gt;
&lt;li&gt;population should be normal&lt;/li&gt;
&lt;li&gt;if not sample size should be large (rule of thumb &amp;gt; 30)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="confidence-interval"&gt;Confidence Interval&lt;/h2&gt;
&lt;p&gt;A confidence interval gives an estimated range of values which is likely to include an unknown population parameter, the estimated range being calculated from a given set of sample data (the interval is a random observed variable depending on the sample).  &lt;/p&gt;
&lt;p&gt;The level &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; of a confidence interval gives the probability that the interval produced by the method employed includes the true value of this parameter.
The level of confidence is the percentage of chance the unknown parameter is contained in the interval (which would differ for each sample across repeated sampling).
The confidence interval represents values for the population parameter for which the difference between this parameter and the observed estimate is not statistically significant at the &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; level.  &lt;/p&gt;
&lt;p&gt;Let X be a random sample from a probability distribution &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, a quantity to be estimated. A confidence interval I for &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; with confidence level &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; has the property:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(\theta \in I(x)) \geq 1 - \alpha\)&lt;/span&gt; with P a density probability depending on theta.&lt;/p&gt;
&lt;p&gt;Let's take the example of the estimation of the mean of a population normally distributed which is the simpleast usage of condidence interval.&lt;br/&gt;
The confidence interval of a sample mean &lt;span class="math"&gt;\(\bar X\)&lt;/span&gt; from a normally distributed sample is also normally distributed (from CLT), with the same expectation &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and a standard deviation :&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\frac {\sigma}{\sqrt{n}}\)&lt;/span&gt; (the standard deviation of a dsitribution of sample means is called standard error, SE)&lt;/p&gt;
&lt;p&gt;By standardizing &lt;span class="math"&gt;\(\bar X\)&lt;/span&gt;, we get a random variable: &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Z = \frac {\bar X-\mu}{\sigma/\sqrt{n}}\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;Z depend on the parameter &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; to be estimated and a standard normal distribution independent of the parameter &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;. We look for numbers -z and z, independent of &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; between which Z lies with a probability &lt;span class="math"&gt;\(1 - \alpha\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;For a 95% of confidence, we take &lt;span class="math"&gt;\(1 - \alpha\)&lt;/span&gt; = 0.95, so&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\!P(-z\le Z\le z) = 1-\alpha = 0.95\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Z follows the cumulative normal distribution (Z is standardized)&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\Phi(z) = P(Z \le z) = 1 - \tfrac{\alpha}{2} = 0.975\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\Phi(z)  = \Phi^{-1}(\Phi(z)) = \Phi^{-1}(0.975) = 1.96\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/NormalDist1.96.png"/&gt;
1.96 correspond to the 95% + 2.5% = 97.5% quantile of the normal distribution, according to the Z table (find 0.975 in the table and add the column header with the row header).
&lt;img alt="" src="figure/6368.png"/&gt;
&lt;img alt="Standard deviation diagram" src="figure/Standard_deviation_diagram.png"/&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.95 = 1-\alpha=P(-z \le Z \le z)=P \left(-1.96 \le \frac {\bar X-\mu}{\sigma/\sqrt{n}} \le 1.96 \right)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(0.95 = P \left( \bar X - 1.96 \frac{\sigma}{\sqrt{n}} \le \mu \le \bar X + 1.96 \frac{\sigma}{\sqrt{n}}\right)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The lower endpoint of the the interval is &lt;span class="math"&gt;\(\bar X - 1.96 \frac{\sigma}{\sqrt{n}}\)&lt;/span&gt; &lt;br/&gt;
The upper endpoint of the the interval is &lt;span class="math"&gt;\(\bar X + 1.96 \frac{\sigma}{\sqrt{n}}\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;We've computed our interval that can be interpreted this way : If we take 100 samples of  size n and for each sample we compute the interval, then the parameter will be in 95 of the intervals computed, and outside of 5 intervals. We are 95% confident ot the result. &lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;p&gt;You have taken a random sample of 100 primary school children. Their heights had mean = 150cm and sd = 10 cm. We estimate the true average height of primary school children based on this sample using a 95% confidence interval.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\bar{x} = z \times SE = x\bar \pm 1.96 \times \frac{s}{\sqrt{n}} = 150 \pm 1.96 \times \frac{10}{\sqrt{100}} = 150 \pm \times 1.96 \times 1 = (148.04, 151.96)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We are 95% confident that primary school children mean height is between 148.04cm and 151.96cm.&lt;/p&gt;
&lt;p&gt;As the standard deviation of the population &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; is known in this case, the distribution of the sample mean &lt;span class="math"&gt;\(\bar X\)&lt;/span&gt; is a normal distribution with &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; the only unknown parameter. In most of practical case, the parameter &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; is also unknown, which calls for using the (Student's) t-distribution.&lt;/p&gt;
&lt;h2 id="required-sample-size-for-margin-of-error"&gt;Required sample size for margin of error&lt;/h2&gt;
&lt;p&gt;Given a target margin of error and confidence level, and the standard deviation of a sample (or population), we can work backwards to determine the required sample size.&lt;/p&gt;
&lt;p&gt;Example :&lt;/p&gt;
&lt;p&gt;From previous measurements of primary school heights. What should be the sample size in order to get a 95% confidence interval with a margin of error less or equal to 1 cm:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(ME = z \times SE\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(1 = 1.96 \times \frac{10}{\sqrt{n}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(n = \left(\frac{1.96 \times 10}{1}\right)^2 = 19.6^2 = 384.16\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus we need a sample size of at least 385 primary school children.&lt;/p&gt;
&lt;h2 id="hypothesis-testing"&gt;Hypothesis testing&lt;/h2&gt;
&lt;p&gt;When interpreting an experimental finding, a natural question arises as to whether the finding could have occurred by chance. Hypothesis testing is a statistical procedure for testing whether chance is a plausible explanation of an experimental finding.&lt;/p&gt;
&lt;p&gt;The researcher has a proposed hypothesis about a population characteristic and conducts a study to discover if it is reasonable, or, acceptable.&lt;/p&gt;
&lt;p&gt;Null Hypothesis &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; : The status quo that is assumed to be true. &lt;br/&gt;
Alternative hypothesis &lt;span class="math"&gt;\(H_a\)&lt;/span&gt; : An alternative claim under consideration that will require statistical evidence to accept, and thus, reject the null hypothesis. The alternative hypothesis claims that the population characteristic is different than the observed parameter. This difference is either that the characteristic has increased, decreased, or, possibly either increased or decreased.&lt;/p&gt;
&lt;p&gt;The alternative hypotheses are typically of the form &amp;lt; (decrease), &amp;gt; (increase) or &lt;span class="math"&gt;\(\neq\)&lt;/span&gt; (either increase or decrease).
We have four possible outcomes:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Truth&lt;/th&gt;
&lt;th&gt;Decide&lt;/th&gt;
&lt;th&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;H0&lt;/td&gt;
&lt;td&gt;H0&lt;/td&gt;
&lt;td&gt;Correctly accept null&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;H0&lt;/td&gt;
&lt;td&gt;Ha&lt;/td&gt;
&lt;td&gt;Type I error (False Positive, falsely claims a significant result)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ha&lt;/td&gt;
&lt;td&gt;Ha&lt;/td&gt;
&lt;td&gt;Correctly reject null&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ha&lt;/td&gt;
&lt;td&gt;H0&lt;/td&gt;
&lt;td&gt;Type II error (False Negative falsely claims a nonsignificant result)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A test statistic is used to make an assumption, the null is made upon this assumption. The test statistic will have a certain likelihood for occurring, according to the distribution being used. When this likelihood is small, this indicates that the sample data are either from an unusual sample, or, that the distribution of the population actually is different than assumed. 
If the sample is properly drawn, there is small risk that the sample is unusual, and, so, it is safe to draw a conclusion that the distribution may be changed.  This allows the conclusion that the null hypothesis may have changed, and that the alternative hypothesis might be accepted instead.  This conclusion leads the researcher to "reject" the null hypothesis.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;p&gt;From previous example, does the data support the hypothesis that primary school children on average are shorter than 151cm?&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(H_0 : \mu_0 = 151\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(H_a : \mu_0 &amp;lt; 151\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Assuming &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;, how unusual or extreme is the sample value we get from our OBSERVED data? or
Assuming &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;, what is the probability to obtain the observed data (ie. a mean of 150cm &amp;lt; 151 cm, with a sd=10cm) or a more extreme values?&lt;/p&gt;
&lt;p&gt;An equivalent question is, assuming &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;, ie. assuming X' normally distributed with &lt;span class="math"&gt;\(X' \sim N(150, 1)\)&lt;/span&gt;, what is the probability to obtain a standard deviation at least that far from the mean? &lt;/p&gt;
&lt;p&gt;We must determine how our hypothesis mean is far from our OBSERVED sample mean (the z-score):&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Z = \frac{\bar{X} - \mu}{\bar{\sigma_X}} = \frac{150 - 151}{1} = -1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Our observed data are -1 standard deviation from the hypothesis mean.&lt;/p&gt;
&lt;p&gt;A reasonable strategy would be : do not reject the null hypothesis, ie. "primary school children have mean height of 151 cm", if there is more than x% chance of getting a random sample of 100 children with a sample mean 150, with x enough hight (more than 5% usually).&lt;/p&gt;
&lt;p&gt;The probability under the null hypothesis of obtaining evidence as or more extreme than your z-score or test statistic (obtained from your observed data) in the direction of the alternative hypothesis is the p-value.&lt;/p&gt;
&lt;h2 id="p-value"&gt;P-value&lt;/h2&gt;
&lt;p&gt;Probability of obtaining the observed result or results that are more "extreme", given that hypothesis is true, ie. P(observed or more extreme outcome | &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;A reasonable strategy would reject the null hypothesis if the sample mean &lt;span class="math"&gt;\(\bar X\)&lt;/span&gt; is larger or lower than some constant C, chosen so that the probability of a Type I error is &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note:
  &lt;span class="math"&gt;\(C = \mu + qnorm(\alpha) \times sd\)&lt;/span&gt;
  instead of computing a constant C as a cutpoint for accepting or rejecting &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;, we simply compute a Z-score based on alpha, the number of standard deviations the sample mean is from the hypothesized mean.&lt;/p&gt;
&lt;p&gt;If the p-value is low (ie. lower than the significant level (&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;), usually 5% as a standard level of rejection), then we saw that is very unlikely to observe the data if the null hypothesis is true and reject it.&lt;/p&gt;
&lt;p&gt;If the p-value is high (ie. higher than (&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;), we say that it is likely to observe the data even if the null hypothesis was true, and thus do not reject it.&lt;/p&gt;
&lt;h2 id="interpreting-the-p-value"&gt;Interpreting the p-value&lt;/h2&gt;
&lt;p&gt;When a probability value is below the &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; level, the effect is statistically significant and the null hypothesis is rejected.&lt;br/&gt;
However, not all statistically significant effects should be treated the same way. For example, you should have less confidence that the null hypothesis is false if p = 0.049 than p = 0.003.&lt;br/&gt;
If the null hypothesis is rejected, then the alternative to the null hypothesis (called the alternative hypothesis) is accepted.  &lt;/p&gt;
&lt;p&gt;In many situations it is very unlikely two conditions will have the same population means. Therefore, even before an experiment comparing their effectiveness is conducted, the researcher knows that the null hypothesis of exactly no difference is false. If a test of the difference is significant, then the direction of the difference is established.&lt;/p&gt;
&lt;p&gt;When a significance test results in a high probability value, it means that the data provide little or no evidence that the null hypothesis is false. However, the high probability value is not evidence that the null hypothesis is true. The problem is that it is impossible to distinguish a null effect from a very small effect.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;p&gt;From the previous example, with a significant level equal to 0.05:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\bar{X} \sim N(\mu = 151, SE = 1)\)&lt;/span&gt; #Null hypothesis&lt;/p&gt;
&lt;p&gt;Test statistic or Z-score:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Z = \frac{\bar X - \mu}{SE} = \frac{150 - 151}{1} = -1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The probability that we are at most -1 standard deviation from the mean:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P\left(Z &amp;lt; -1\right) = 1 - 0.8413 = 0.1587\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This probability can be computed with the qnorm R function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;pnorm&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;-1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## [1] 0.1586553&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we assume &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; (&lt;span class="math"&gt;\(mu = 151\)&lt;/span&gt;), the probability of getting a sample this "extreme" (&lt;span class="math"&gt;\(\mu = 150\)&lt;/span&gt;) or actually more extreme is 15.9%.
Since p-value is higher than 5%, we don't to reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Interpretation :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If in fact, primary school children have mean height of 151 cm, there is a 15,9% chance that a random sample of 100 children would yield a sample mean of 150cm or lower.&lt;/li&gt;
&lt;li&gt;This is a pretty hight probability&lt;/li&gt;
&lt;li&gt;Thus the sample mean of 150 could have likely occured by chance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="two-sided-hypothesis-testing"&gt;Two-sided Hypothesis testing&lt;/h2&gt;
&lt;p&gt;The test above was a one-side or one-tailed test.
What is the probability that the children have mean height different from 151cm?&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(H_0 : \mu = 151\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(H_a : \mu \neq 151\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We could reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; (and accept &lt;span class="math"&gt;\(H_a\)&lt;/span&gt;) when our sample mean is significant different that 151, that is either less than OR greater that 151. 
We consider values at both tails at the .025 and the .975 percentiles.
This means that the test statistic is less than .025, Z_(alpha/2), or greater than .975, Z_(1-alpha/2).
Notice that if we reject H_0, either it was FALSE (and hence our model is wrong and we are correct to reject it) OR H_0 is TRUE and we have made an error (Type I). The probability of this is 5%.&lt;/p&gt;
&lt;p&gt;P-value:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P\left(Z &amp;lt; -1\right) +  P\left(Z &amp;gt; 1\right) = 2 \times (1-0.8413) = 0.3174\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With R:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; pnorm&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;-1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## [1] 0.3173105&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="decision-rule"&gt;Decision rule&lt;/h2&gt;
&lt;p&gt;The decision rule is to reject the null hypothesis H0 if the observed value is in the critical region, and to accept or "fail to reject" the hypothesis otherwise.&lt;/p&gt;
&lt;p&gt;Left Tailed Test: &lt;br/&gt;
&lt;span class="math"&gt;\(H_0 : \mu = \mu_0\)&lt;/span&gt; parameter = value &lt;br/&gt;
&lt;span class="math"&gt;\(H_a : \mu &amp;lt; \mu_0\)&lt;/span&gt; parameter  &amp;lt; value &lt;br/&gt;
with alpha = 0.5 &lt;br/&gt;
Reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;, if the test statistics is in the region of rejection, ie. if it is smaller than Z_5.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;Z_95 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; qnorm&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0.95&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;Z_95&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## [1] 1.644854&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;the 95% percentile corresponds to the value 1.64 (see also zthe z table above).&lt;/p&gt;
&lt;div class="rimage center"&gt;&lt;img alt="plot of chunk Z_5" class="plot" src="figure/Z_5-1.png" title="plot of chunk Z_5"/&gt;&lt;/div&gt;
&lt;p&gt;Right Tailed Test: &lt;br/&gt;
&lt;span class="math"&gt;\(H_0 : \mu = \mu_0\)&lt;/span&gt; parameter = value &lt;br/&gt;
&lt;span class="math"&gt;\(H_a : \mu &amp;gt; \mu_0\)&lt;/span&gt; parameter &amp;gt; value &lt;br/&gt;
Reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;, if the test statistics is in the region of rejection, ie. if it is larger than Z_95.   &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;Z_5 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; qnorm&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;Z_5&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## [1] -1.644854&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="rimage center"&gt;&lt;img alt="plot of chunk Z_95" class="plot" src="figure/Z_95-1.png" title="plot of chunk Z_95"/&gt;&lt;/div&gt;
&lt;p&gt;Two Tailed Test: &lt;br/&gt;
&lt;span class="math"&gt;\(H_0 : \mu = \mu_0\)&lt;/span&gt; parameter = value &lt;br/&gt;
&lt;span class="math"&gt;\(H_a : \mu \neq \mu_0\)&lt;/span&gt; parameter &lt;span class="math"&gt;\(\neq\)&lt;/span&gt; value  (Another way to write not equal is &amp;lt; or &amp;gt;) &lt;br/&gt;
Reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;, if the test statistics is in the region of rejection, ie. if it is larger than Z_95 or smaller than Z_5.   &lt;/p&gt;
&lt;p&gt;The decision rule can be summarized as follows:&lt;/p&gt;
&lt;p&gt;Reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; if the test statistic falls in the critical region (reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; if the test statistic is more extreme than the critical value or reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; ), otherwise, we fail to reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The p-value tells us if the test statistic is inside our outside the region. &lt;br/&gt;
Reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; if p-value is less that the specified &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, otherwise, we fail to reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note : I you fail to reject the one sided test, you know that you will fail to reject the two sided.&lt;/p&gt;
&lt;h2 id="hypothesis-tests-and-confidence-intervals"&gt;Hypothesis tests and Confidence Intervals&lt;/h2&gt;
&lt;p&gt;They're equivalent.&lt;br/&gt;
If you set &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; to some value and ran many tests checking alternative hypotheses against &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; that &lt;span class="math"&gt;\(\mu=\mu_0\)&lt;/span&gt;, the set of all possible values for which you fail to reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; forms the &lt;span class="math"&gt;\((1\alpha)%\)&lt;/span&gt; (that is 95%) confidence interval for &lt;span class="math"&gt;\(\mu_0\)&lt;/span&gt;.
Similarly, if a &lt;span class="math"&gt;\((1\alpha)%\)&lt;/span&gt; interval contains mu_0, then we fail to reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So, to resume:
If the confidence interval contains the null value (&lt;span class="math"&gt;\(\mu_0\)&lt;/span&gt;, the value of &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;), don't reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;.
If the confidence interval does not contain the null value, reject &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;, cause this tells us that either our hypothesis is wrong or we're making a mistake (Type 1) in rejecting it.&lt;/p&gt;
&lt;p&gt;Previously, we found the 95% interval for heights of primary school children to be (148, 152). Given that our null hypothesis (&lt;span class="math"&gt;\(H_0 = 151\)&lt;/span&gt;) falls within this 95% Cl, we do not reject it.&lt;/p&gt;
&lt;pre&gt;&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;              &amp;lt;- 95% confident that the av is somewherer in here -&amp;gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    ---------|-----------------------------------------------------|---------&lt;/span&gt;
&lt;span class="code-line"&gt;             148cm                                                 152&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;A two-sided hypothesis with significance level &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is equivalent to a confidence interval with &lt;span class="math"&gt;\(CL = 1 - \alpha\)&lt;/span&gt;. 
A one-sided hypothesis with significance level &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is equivalent to a confidence interval with &lt;span class="math"&gt;\(CL = 1 - 2\alpha\)&lt;/span&gt;. &lt;/p&gt;
&lt;h2 id="type-ii-error"&gt;Type II error&lt;/h2&gt;
&lt;p&gt;Let the probability of a type II error (accepting H_0 when it is false) to be beta.
The term POWER refers to the quantity 1-beta and it represents the probability of rejecting &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; when it's false. This is used to determine appropriate sample sizes in experiments.&lt;/p&gt;
&lt;h2 id="the-t-distribution"&gt;The t distribution&lt;/h2&gt;
&lt;p&gt;So far, we use normal distribution and implicitly relying on the Central Limit Theorem.
According to CLT, the distribution of sample statistics is approximatively normal, if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Population is normal&lt;/li&gt;
&lt;li&gt;Sample size is large (n&amp;gt; 30)
If so, we can use the population sd (&lt;span class="math"&gt;\(s\sigma\)&lt;/span&gt;) to compute a z-score.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, when we deal with small sample size and do not know the standard deviation of the population (&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;), we rely on the t distribution.&lt;br/&gt;
The t distribution takes into account that spread of possible &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;'s.&lt;br/&gt;
The test statistic is the same as before &lt;span class="math"&gt;\(\frac{Observed-Expected}{SE}\)&lt;/span&gt; ie. &lt;span class="math"&gt;\(\frac{Observed-Expected}{s/\sqrt{n}}\)&lt;/span&gt; and the test statistic is compared to &lt;span class="math"&gt;\(t_{1-\alpha, df}\)&lt;/span&gt; or/and &lt;span class="math"&gt;\(t_{\alpha, df}\)&lt;/span&gt; (with df the degree of freedom = size - 1).&lt;/p&gt;
&lt;p&gt;Shape of the distribution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Observations are more likely to fall beyond 2 sd from the mean&lt;/li&gt;
&lt;li&gt;The thicker tails are helpful in adjusting for the less reliable data on the standard deviation.
The t distribution has one parameter, degrees of freedom (df) which determines the thickness of the tail&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Under &lt;span class="math"&gt;\(H_0\)&lt;/span&gt;, the probability that the test statistic is larger than the 95th percentile of the t distribution is 5%. The associated quantile is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;n &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;16&lt;/span&gt;  &lt;span class="c1"&gt;# sample size&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;pt&lt;span class="p"&gt;(&lt;/span&gt;q &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; df &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; lower.tail &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;FALSE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## [1] 0.0122529&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;the P(X &amp;gt; 2.5) if &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; were true. We would see this large a test statistic with probability 1% which is rather a small probability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;df refers to the number of independent observations in data set&lt;/li&gt;
&lt;li&gt;nb of independent observations = sample size - 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When df increases, the t distribution approaches the normal distribution.&lt;/p&gt;
&lt;p&gt;Normal distribution vs t distribution: if you're unsure which one to use, use the t distribution since it approximates to the normal distribution with large sample sizes.&lt;/p&gt;
&lt;p&gt;T table
&lt;img alt="" src="figure/t-table.jpg"/&gt;&lt;/p&gt;
&lt;h2 id="independent-and-dependent-t-tests"&gt;Independent and dependent t-tests&lt;/h2&gt;
&lt;p&gt;Dependent t-test: when evaluating the effect between two related samples, ie.  when the same subjects are being compared or when two samples are matched at the level of individual subjects.&lt;br/&gt;
Example : You feed a group of 100 people fast food everyday, did they gain weight?&lt;br/&gt;
You can calculate a difference score and then determine if the mean difference score is significantly different from zero and so if there is significantly change.&lt;/p&gt;
&lt;p&gt;Independent t-test: when evaluating the effect between two independent sample: You feed 50 males and 50 males fast food everyday. Did males or females gain more weight after 30 days?&lt;/p&gt;
&lt;h2 id="test-comparisons"&gt;Test Comparisons&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;-&lt;/th&gt;
&lt;th&gt;Observed&lt;/th&gt;
&lt;th&gt;Expected&lt;/th&gt;
&lt;th&gt;SE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;z&lt;/td&gt;
&lt;td&gt;Sample mean&lt;/td&gt;
&lt;td&gt;Pop. mean&lt;/td&gt;
&lt;td&gt;SE of the mean&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;t (single sample)&lt;/td&gt;
&lt;td&gt;Sample mean&lt;/td&gt;
&lt;td&gt;Pop. mean&lt;/td&gt;
&lt;td&gt;SE of the mean&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;t(dependent)&lt;/td&gt;
&lt;td&gt;Sample mean of &lt;br&gt; difference scores&lt;/br&gt;&lt;/td&gt;
&lt;td&gt;Pop. mean of &lt;br&gt; difference scores&lt;/br&gt;&lt;/td&gt;
&lt;td&gt;SE of the mean difference&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;t(independent)&lt;/td&gt;
&lt;td&gt;Difference between &lt;br&gt; two sample means&lt;/br&gt;&lt;/td&gt;
&lt;td&gt;Difference between &lt;br&gt; two pop. mean&lt;/br&gt;&lt;/td&gt;
&lt;td&gt;SE of the difference between means&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
Test Statistics (in each case, the test statistics = (Observed - Expected) / SE)

-               | value            | SE
----------------|------------------|------------------------
z               |  M - M0 / SE     | S / &amp;radic;N
t(single sample)|                  |  
t(dependent)    |  M - 0 / SE      | 
t(independent)  | (M1 - M2)/SE     | (SE1 + SE2)/2 
--&gt;
&lt;p&gt;Degrees of freedom:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;-&lt;/th&gt;
&lt;th&gt;df&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;z&lt;/td&gt;
&lt;td&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;t (single sample)&lt;/td&gt;
&lt;td&gt;N-1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;t(dependent)&lt;/td&gt;
&lt;td&gt;N-1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;t(independent)&lt;/td&gt;
&lt;td&gt;(N1-1) + (N2-1)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="using-the-r-ttest-function"&gt;Using the R t.test function&lt;/h2&gt;
&lt;h3 id="mean-of-difference"&gt;Mean of difference&lt;/h3&gt;
&lt;p&gt;From the father.son library, which contains 1078 measurements of a father's height and his son's height, we test the mean of the difference of the vectors sheight (son height) and fheight (father height), the null hypothesis is the true mean of the difference is 0.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"UsingR"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="p"&gt;(&lt;/span&gt;father.son&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;t.test&lt;span class="p"&gt;(&lt;/span&gt;father.son&lt;span class="o"&gt;$&lt;/span&gt;sheight &lt;span class="o"&gt;-&lt;/span&gt; father.son&lt;span class="o"&gt;$&lt;/span&gt;fheight&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;  One Sample t-test&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; data:  father.son$sheight - father.son$fheight&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; t = 11.789, df = 1077, p-value &amp;lt; 2.2e-16&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; alternative hypothesis: true mean is not equal to 0&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; 95 percent confidence interval:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;  0.8310296 1.1629160&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; sample estimates:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; mean of x &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; 0.9969728&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As said before, t = &lt;span class="math"&gt;\(\frac{X'-\mu}{SE}\)&lt;/span&gt; ie. &lt;span class="math"&gt;\(\frac{X'- \mu}{s/\sqrt{n}}\)&lt;/span&gt;&lt;br/&gt;
&lt;span class="math"&gt;\(\mu = 0\)&lt;/span&gt; as we test difference of means.
We can check that t = 11.789 as returned by the t.test function,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;v &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; father.son&lt;span class="o"&gt;$&lt;/span&gt;sheight &lt;span class="o"&gt;-&lt;/span&gt; father.son&lt;span class="o"&gt;$&lt;/span&gt;fheight&lt;/span&gt;
&lt;span class="code-line"&gt;t &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;v&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;sd&lt;span class="p"&gt;(&lt;/span&gt;v&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kp"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;v&lt;span class="p"&gt;)))&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;t&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;[1] 11.78855&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;confidence intervals:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;v&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;-1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;sd&lt;span class="p"&gt;(&lt;/span&gt;v&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kp"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;v&lt;span class="p"&gt;)))&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="difference-in-means"&gt;Difference in means&lt;/h3&gt;
&lt;p&gt;We test the difference in means of the vectors sheight and fheight, the null hypothesis is the true difference in means is 0.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;t.test&lt;span class="p"&gt;(&lt;/span&gt;father.son&lt;span class="o"&gt;$&lt;/span&gt;sheight&lt;span class="p"&gt;,&lt;/span&gt; father.son&lt;span class="o"&gt;$&lt;/span&gt;fheight&lt;span class="p"&gt;,&lt;/span&gt; paired &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;  Paired t-test&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; data:  father.son$sheight and father.son$fheight&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; t = 11.789, df = 1077, p-value &amp;lt; 2.2e-16&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; alternative hypothesis: true difference in means is not equal to 0&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; 95 percent confidence interval:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;  0.8310296 1.1629160&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; sample estimates:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; mean of the differences &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;               0.9969728&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The test statistic is 11.789 which is quite hight so we reject the null hypothesis that the true mean of the difference is 0, (if you ran the test on the difference sheight-fheight) or that the true difference in means was 0 (if you ran the test on the two separate but paired columns).&lt;/p&gt;
&lt;p&gt;Note the 95% confidence interval, 0.8310296 1.1629160, returned by t.test. It does not contain the hypothesized population mean 0 so we're pretty confident we can safely reject the hypothesis. This tells us that either our hypothesis is wrong or we're making a mistake (Type 1) in rejecting it.&lt;/p&gt;
&lt;!--

Drawing conclusions comparing the probability value ( the probability of obtaining a sample statistic as different or more different from the parameter specified in the null hypothesis given that the null hypothesis is true.) with the $\alpha$ level. If the probability value is lower then you reject the null hypothesis. Keep in mind that rejecting the null hypothesis is not an all-or-none decision. The lower the probability value, the more confidence you can have that the null hypothesis is false. However, if your probability value is higher than the conventional $\alpha$ level of 0.05, most scientists will consider your findings inconclusive. Failure to reject the null hypothesis does not constitute support for the null hypothesis. It just means you do not have sufficiently strong data to reject it. 

There is a close relationship between confidence intervals and significance tests. Specifically, if a statistic is significantly different from 0 at the 0.05 level, then the 95% confidence interval will not contain 0. All values in the confidence interval are plausible values for the parameter, whereas values outside the interval are rejected as plausible values for the parameter


Confidence intervals are closely related to statistical significance testing.
Hypothesis testing is concerned with making decisions using data. It compares the data being studied to an observed characteristic of the population from which the data are sampled.

The 5% is considered as the region of rejection, if the z score is outside the region of rejection (determined by $alpha$), we fail to reject $H_0$.

The p-value is the probability under the null hypothesis of obtaining evidence as or more extreme than your z-score or test statistic (obtained from your observed data) in the direction of the alternative hypothesis.
So if the p-value (probability of seeing your test statistic) is small, then one of two things happens. EITHER $H_0$ is true and you have observed a rare event (in this unusual test statistic) OR $H_0$ is false.

The p-value is as an attained significance level, ie. the smallest value of alpha at which you will reject the null hypothesis.

--&gt;
&lt;!--
Sources 
http://statstutorstl.blogspot.fr/search/label/inferential%20statistics
http://fr.slideshare.net/eugeneyan/statistical-inference-3
http://www.stat.yale.edu/Courses/1997-98/101/confint.htm
--&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="R"></category><category term="stats"></category></entry><entry><title>River Crossing Puzzle</title><link href="http://stephanie-w.github.io/blog/crossing_river.html" rel="alternate"></link><updated>2015-07-21T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-07-21:blog/crossing_river.html</id><summary type="html">&lt;hr /&gt;
&lt;p&gt;A river crossing puzzle is a type of transport puzzle in which the object is to carry items from one river bank to another. The difficulty of the puzzle may arise from restrictions on which or how many items can be transported at the same time, or from which or how many items may be safely left together. The setting may vary cosmetically, for example, by replacing the river by a bridge.&lt;/p&gt;
&lt;p&gt;The story:&lt;/p&gt;
&lt;p&gt;Once upon a time a farmer went to market and purchased a fox, a goose, and a bag of beans. On his way home, the farmer came to the bank of a river and rented a boat. But in crossing the river by boat, the farmer could carry only himself and a single one of his purchases - the fox, the goose, or the bag of beans.&lt;/p&gt;
&lt;p&gt;If left together, the fox would eat the goose, or the goose would eat the beans.&lt;/p&gt;
&lt;p&gt;The farmer's challenge was to carry himself and his purchases to the far bank of the river, leaving each purchase intact. How did he do it?&lt;/p&gt;
&lt;p&gt;Graph-Theoretic Analysis:&lt;/p&gt;
&lt;p&gt;The farmer (T for transporter) must transport a fox (F), a goose (G) and a bag of beans (B) from one side of a river to another using a boat which can only hold one item in addition to the farmer. For edibility reasons, the fox cannot be left alone with the goose, and the goose cannot be left alone with the beans.
So we can't have F and G together and G and B together.&lt;/p&gt;
&lt;p&gt;To draw the graph, let's have graph nodes listing items on one side of the river (T F G B), the edges will be the transport transition. The only possibles transitions are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;* TFGB -&amp;gt; FB&lt;/span&gt;
&lt;span class="code-line"&gt;* TFB (the transporter comes back) -&amp;gt; B&lt;/span&gt;
&lt;span class="code-line"&gt;                                   -&amp;gt; F&lt;/span&gt;
&lt;span class="code-line"&gt;* B -&amp;gt; TGB&lt;/span&gt;
&lt;span class="code-line"&gt;* F -&amp;gt; TFG&lt;/span&gt;
&lt;span class="code-line"&gt;Then we always fing G alone, then TG, then nobody&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The departure river side graph is :&lt;/p&gt;
&lt;p&gt;&lt;img alt="Departure River Side" src="figure/river-crossing-departure-en.png" /&gt;&lt;/p&gt;
&lt;p&gt;The arrival river side graph is :
&lt;img alt="Arrival River Side" src="figure/river-crossing-arrival-en.png" /&gt;&lt;/p&gt;
&lt;p&gt;These two graphs are inverted (right/left inversion from the other one), as there is only one graph of possible transitions, which is the same at the both river sides.
You can draw them simultaneously as two nodes at the same position on the graph must contains all items.&lt;/p&gt;
&lt;p&gt;There is two solutions to the problem, after leaving the goose alone on the arrival river side, the farmer can leave either the fox either the beans on the departure river side.&lt;/p&gt;</summary></entry><entry><title>R : Variance Inflation</title><link href="http://stephanie-w.github.io/blog/variance-inflation.html" rel="alternate"></link><updated>2015-07-18T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-07-18:blog/variance-inflation.html</id><summary type="html">&lt;hr /&gt;
&lt;p&gt;This is my note on swirl course Regression Model : Overfitting and Underfitting.&lt;/p&gt;
&lt;h2 id="definition"&gt;Definition&lt;/h2&gt;
&lt;p&gt;&lt;!-- BEGIN_SUMMARY --&gt;
A variance inflation factor (VIF) is a ratio of estimated variances, the variance due to including the ith regressor, divided by that due to including a corresponding ideal regressor which is uncorrelated with the others.
VIF is the square of standard error inflation.&lt;/p&gt;
&lt;!-- END_SUMMARY --&gt;

&lt;p&gt;More simply, it estimates how much the variance of a coefficient is "inflated" because of linear dependence with other predictors. Thus, a VIF of 1.8 tells us that the variance of a particular coefficient is 80% larger than it would be if that predictor was completely uncorrelated with all the other predictors.&lt;br /&gt;
The VIF has a lower bound of 1 but no upper bound. &lt;/p&gt;
&lt;h2 id="examples-with-the-swiss-dataset"&gt;Examples with the swiss dataset&lt;/h2&gt;
&lt;p&gt;To explore VIF, we'll use the Swiss Fertility and Socioeconomic Indicators (1888) dataset which reports standardized fertility measure and socio-economic indicators for each of 47 French-speaking provinces of Switzerland at about 1888. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;data&lt;span class="p"&gt;(&lt;/span&gt;swiss&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;swiss&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We fit a model with Fertility as outcome and use the R's function vif (from the car package) to compute variance inflations:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;data&lt;span class="p"&gt;(&lt;/span&gt;swiss&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;swiss&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;mdl &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;Fertility &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="m"&gt;.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; swiss&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;car&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;vif&lt;span class="p"&gt;(&lt;/span&gt;mdl&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##      Agriculture      Examination        Education         Catholic Infant.Mortality &lt;/span&gt;
&lt;span class="code-line"&gt;##         2.284129         3.675420         2.774943         1.937160         1.107542&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For each regression coefficient, the variance inflation due to including all the others.&lt;br /&gt;
For instance, the variance in the estimated coefficient of Education is 2.774943 times what it might have been if Education were not correlated with the other regressors.&lt;br /&gt;
We can guess that Examination and Education are likely to be correlated, so most of the variance inflation for Education is due to including Examination.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;mdl2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;Fertility &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="m"&gt;.&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; Examination&lt;span class="p"&gt;,&lt;/span&gt; swiss&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;vif&lt;span class="p"&gt;(&lt;/span&gt;mdl2&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##      Agriculture        Education         Catholic Infant.Mortality &lt;/span&gt;
&lt;span class="code-line"&gt;##         2.147153         1.816361         1.299916         1.107528&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As expected, omitting Examination in the model decreased the VIF for Education, from 2.774943 to 1.816361. Notice it has almost no effect on the VIF for Infant Mortality.&lt;/p&gt;
&lt;p&gt;Including new variables to a model will increase standard errors of coefficient estimates of other correlated refressors. On the other hand, omitting varaibles results can bias in coefficients of regressors which are correlated with the omitted one.&lt;/p&gt;
&lt;p&gt;Analysis of variance (ANOVA) is a useful way to quantify the significance of additional regressors.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;fit1 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;Fertility &lt;span class="o"&gt;~&lt;/span&gt; Agriculture&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; swiss&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;fit3 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;Fertility &lt;span class="o"&gt;~&lt;/span&gt; Agriculture &lt;span class="o"&gt;+&lt;/span&gt; Examination &lt;span class="o"&gt;+&lt;/span&gt; Education&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; swiss&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;anova&lt;span class="p"&gt;(&lt;/span&gt;fit1&lt;span class="p"&gt;,&lt;/span&gt; fit3&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## Analysis of Variance Table&lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Model 1: Fertility ~ Agriculture&lt;/span&gt;
&lt;span class="code-line"&gt;## Model 2: Fertility ~ Agriculture + Examination + Education&lt;/span&gt;
&lt;span class="code-line"&gt;##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    &lt;/span&gt;
&lt;span class="code-line"&gt;## 1     45 6283.1                                  &lt;/span&gt;
&lt;span class="code-line"&gt;## 2     43 3180.9  2    3102.2 20.968 4.407e-07 ***&lt;/span&gt;
&lt;span class="code-line"&gt;## ---&lt;/span&gt;
&lt;span class="code-line"&gt;## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The null hypothesis is rejected at the 0.001 level based on a right-tailed F test (F value=20.968).  &lt;/p&gt;
&lt;p&gt;RSS (Residual sum of squares) are 6283.1 and 3180.9.&lt;br /&gt;
We can check the results with the R's deviance function, which calculate the residual sum of squares:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;deviance&lt;span class="p"&gt;(&lt;/span&gt;fit1&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## [1] 6283.116&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;deviance&lt;span class="p"&gt;(&lt;/span&gt;fit3&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## [1] 3180.925&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The F statistic is the ratio of the two sums of squares divided by their respective degrees of freedom.&lt;br /&gt;
For the F value computing, this is the ratio of the difference of deviance divided by the difference in the residual degrees of freedom of fit1 and fit3 (2) and the fit3's residual sum of squares divided by its degrees of freedom. fit3 has 43 residual degrees of freedom (47 number of samples - 4 predictors (the 3 named and the intercept)):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;n &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;deviance&lt;span class="p"&gt;(&lt;/span&gt;fit1&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; deviance&lt;span class="p"&gt;(&lt;/span&gt;fit3&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;d &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; deviance&lt;span class="p"&gt;(&lt;/span&gt;fit3&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;43&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;n&lt;span class="o"&gt;/&lt;/span&gt;d&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## [1] 20.96783&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If the two scaled sums are independent and centrally chi-squared distributed with the same variance, the statistic will have an F distribution with parameters given by the two degrees of freedom.&lt;/p&gt;
&lt;p&gt;For the p-value is the probability that a value of n/d or larger would be drawn from an F distribution which has parameters 2 and 43. The p-value is 4.407e-07, a very unlikely value if the null hypothesis vwere true.&lt;/p&gt;
&lt;p&gt;The p-value can be computed with the R's pf function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;pf&lt;span class="p"&gt;(&lt;/span&gt;n&lt;span class="o"&gt;/&lt;/span&gt;d&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;43&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; lower.tail &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;FALSE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## [1] 4.406913e-07&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Based on the calculated p-value, a false rejection of the null hypothesis is extremely unlikely. We are confident that fit3 is significantly better than fit1, with one caveat: analysis of variance is sensitive to its assumption that model residuals are approximately normal.&lt;br /&gt;
If they are not, we could get a small p-value for that reason.&lt;br /&gt;
It is thus worth testing residuals for normality. The Shapiro-Wilk test tests the residual of fit 3. Normality is its null hypothesis.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;shapiro.test&lt;span class="p"&gt;(&lt;/span&gt;fit3&lt;span class="o"&gt;$&lt;/span&gt;residuals&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;  Shapiro-Wilk normality test&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; data:  fit3$residuals&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; W = 0.97276, p-value = 0.336&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Shapiro-Wilk p-value of 0.336 fails to reject normality, supporting confidence in the analysis of variance.&lt;/p&gt;
&lt;p&gt;We can go on with ANOVA and other variables:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;fit5 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;Fertility &lt;span class="o"&gt;~&lt;/span&gt; Agriculture &lt;span class="o"&gt;+&lt;/span&gt; Examination &lt;span class="o"&gt;+&lt;/span&gt; Education &lt;span class="o"&gt;+&lt;/span&gt; Catholic&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; swiss&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;fit6 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;Fertility &lt;span class="o"&gt;~&lt;/span&gt; Agriculture &lt;span class="o"&gt;+&lt;/span&gt; Examination &lt;span class="o"&gt;+&lt;/span&gt; Education &lt;span class="o"&gt;+&lt;/span&gt; Catholic &lt;span class="o"&gt;+&lt;/span&gt; Infant.Mortality&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; swiss&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;anova&lt;span class="p"&gt;(&lt;/span&gt;fit1&lt;span class="p"&gt;,&lt;/span&gt; fit3&lt;span class="p"&gt;,&lt;/span&gt; fit5&lt;span class="p"&gt;,&lt;/span&gt; fit6&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## Analysis of Variance Table&lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Model 1: Fertility ~ Agriculture&lt;/span&gt;
&lt;span class="code-line"&gt;## Model 2: Fertility ~ Agriculture + Examination + Education&lt;/span&gt;
&lt;span class="code-line"&gt;## Model 3: Fertility ~ Agriculture + Examination + Education + Catholic&lt;/span&gt;
&lt;span class="code-line"&gt;## Model 4: Fertility ~ Agriculture + Examination + Education + Catholic + &lt;/span&gt;
&lt;span class="code-line"&gt;##     Infant.Mortality&lt;/span&gt;
&lt;span class="code-line"&gt;##   Res.Df    RSS Df Sum of Sq       F    Pr(&amp;gt;F)    &lt;/span&gt;
&lt;span class="code-line"&gt;## 1     45 6283.1                                   &lt;/span&gt;
&lt;span class="code-line"&gt;## 2     43 3180.9  2   3102.19 30.2107 8.638e-09 ***&lt;/span&gt;
&lt;span class="code-line"&gt;## 3     42 2513.8  1    667.13 12.9937 0.0008387 ***&lt;/span&gt;
&lt;span class="code-line"&gt;## 4     41 2105.0  1    408.75  7.9612 0.0073357 ** &lt;/span&gt;
&lt;span class="code-line"&gt;## ---&lt;/span&gt;
&lt;span class="code-line"&gt;## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It appears that each model is a significant improvement on its predecessor.&lt;/p&gt;
&lt;!-- 
## Experimenting VIF high values

Regardless of your criterion for what constitutes a high VIF, there are at least three situations in which a high VIF is not a problem and can be safely ignored:

2. The high VIFs are caused by the inclusion of powers or products of other variables.

Sources
http://statisticalhorizons.com/multicollinearity
--&gt;</summary><category term="R"></category><category term="stats"></category></entry><entry><title>R: Multivariable Regression</title><link href="http://stephanie-w.github.io/blog/multivariable-regression.html" rel="alternate"></link><updated>2015-07-12T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-07-12:blog/multivariable-regression.html</id><summary type="html">&lt;hr /&gt;
&lt;p&gt;&lt;!-- BEGIN_SUMMARY --&gt;
&lt;!-- END_SUMMARY --&gt;
This is my note on swirl course Regression Model : Multivariable Examples 3. &lt;/p&gt;
&lt;p&gt;We'll use the hunger dataset from the course:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;download.file&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;https://raw.githubusercontent.com/swirldev/swirl_courses/master/Regression_Models/MultiVar_Examples3/hunger.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="s"&gt;&amp;quot;hunger.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; method &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;curl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; quiet &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;hunger &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;hunger.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;hunger&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##    X                              Indicator Data.Source PUBLISH.STATES Year WHO.region  Country&lt;/span&gt;
&lt;span class="code-line"&gt;## 1  8 Children aged &amp;lt;5 years underweight (%) NLIS_310044      Published 1986     Africa  Senegal&lt;/span&gt;
&lt;span class="code-line"&gt;## 2 11 Children aged &amp;lt;5 years underweight (%) NLIS_310095      Published 1989     Africa   Uganda&lt;/span&gt;
&lt;span class="code-line"&gt;## 3 13 Children aged &amp;lt;5 years underweight (%) NLIS_310138      Published 1988     Africa Zimbabwe&lt;/span&gt;
&lt;span class="code-line"&gt;## 4 16 Children aged &amp;lt;5 years underweight (%) NLIS_310044      Published 1986     Africa  Senegal&lt;/span&gt;
&lt;span class="code-line"&gt;## 5 18 Children aged &amp;lt;5 years underweight (%) NLIS_310095      Published 1989     Africa   Uganda&lt;/span&gt;
&lt;span class="code-line"&gt;## 6 21 Children aged &amp;lt;5 years underweight (%) NLIS_310138      Published 1988     Africa Zimbabwe&lt;/span&gt;
&lt;span class="code-line"&gt;##      Sex Display.Value Numeric Low High Comments&lt;/span&gt;
&lt;span class="code-line"&gt;## 1   Male          19.3    19.3  NA   NA       NA&lt;/span&gt;
&lt;span class="code-line"&gt;## 2 Female          19.1    19.1  NA   NA       NA&lt;/span&gt;
&lt;span class="code-line"&gt;## 3 Female           7.2     7.2  NA   NA       NA&lt;/span&gt;
&lt;span class="code-line"&gt;## 4 Female          15.3    15.3  NA   NA       NA&lt;/span&gt;
&lt;span class="code-line"&gt;## 5   Male          20.4    20.4  NA   NA       NA&lt;/span&gt;
&lt;span class="code-line"&gt;## 6   Male           8.7     8.7  NA   NA       NA&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Numeric column gives the percentage of children under age 5 who were underweight when that sample was taken.&lt;br /&gt;
We fit a simple linear regression for Numeric (outcome) on Year:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;fit &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;Numeric &lt;span class="o"&gt;~&lt;/span&gt; Year&lt;span class="p"&gt;,&lt;/span&gt; hunger&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;fit&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Call:&lt;/span&gt;
&lt;span class="code-line"&gt;## lm(formula = Numeric ~ Year, data = hunger)&lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Residuals:&lt;/span&gt;
&lt;span class="code-line"&gt;##     Min      1Q  Median      3Q     Max &lt;/span&gt;
&lt;span class="code-line"&gt;## -24.621 -11.196  -1.994   7.085  45.039 &lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Coefficients:&lt;/span&gt;
&lt;span class="code-line"&gt;##              Estimate Std. Error t value Pr(&amp;gt;|t|)    &lt;/span&gt;
&lt;span class="code-line"&gt;## (Intercept) 634.47966  121.14460   5.237 2.01e-07 ***&lt;/span&gt;
&lt;span class="code-line"&gt;## Year         -0.30840    0.06053  -5.095 4.21e-07 ***&lt;/span&gt;
&lt;span class="code-line"&gt;## ---&lt;/span&gt;
&lt;span class="code-line"&gt;## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Residual standard error: 13.23 on 946 degrees of freedom&lt;/span&gt;
&lt;span class="code-line"&gt;## Multiple R-squared:  0.02671,    Adjusted R-squared:  0.02568 &lt;/span&gt;
&lt;span class="code-line"&gt;## F-statistic: 25.96 on 1 and 946 DF,  p-value: 4.209e-07&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As time goes on, the rate of hunger decreases.&lt;br /&gt;
The intercept of the model represents the percentage of hungry children at year 0.  &lt;/p&gt;
&lt;p&gt;Let's look at the rates of hunger for the different genders to see how, or even if, they differ:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;lmF &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;Numeric&lt;span class="p"&gt;[&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Sex &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Female&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; Year&lt;span class="p"&gt;[&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Sex &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Female&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; hunger&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;lmF&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Call:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; lm(formula = Numeric[hunger$Sex == &amp;quot;Female&amp;quot;] ~ Year[hunger$Sex == &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;     &amp;quot;Female&amp;quot;], data = hunger)&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Residuals:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;     Min      1Q  Median      3Q     Max &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; -23.228 -10.638  -1.959   6.859  46.146 &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Coefficients:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;                               Estimate Std. Error t value Pr(&amp;gt;|t|)    &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; (Intercept)                  603.50580  167.53201   3.602 0.000349 ***&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Year[hunger$Sex == &amp;quot;Female&amp;quot;]  -0.29340    0.08371  -3.505 0.000500 ***&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; ---&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Residual standard error: 12.94 on 472 degrees of freedom&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Multiple R-squared:  0.02537,    Adjusted R-squared:  0.0233 &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; F-statistic: 12.29 on 1 and 472 DF,  p-value: 5e-04&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;lmM &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;Numeric&lt;span class="p"&gt;[&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Sex &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Male&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; Year&lt;span class="p"&gt;[&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Sex &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Male&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; hunger&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;lmM&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Call:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; lm(formula = Numeric[hunger$Sex == &amp;quot;Male&amp;quot;] ~ Year[hunger$Sex == &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;     &amp;quot;Male&amp;quot;], data = hunger)&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Residuals:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;     Min      1Q  Median      3Q     Max &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; -25.913 -11.741  -1.832   7.399  42.255 &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Coefficients:&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt;                             Estimate Std. Error t value Pr(&amp;gt;|t|)    &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; (Intercept)                665.45352  174.50726   3.813 0.000155 ***&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Year[hunger$Sex == &amp;quot;Male&amp;quot;]  -0.32340    0.08719  -3.709 0.000233 ***&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; ---&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Residual standard error: 13.48 on 472 degrees of freedom&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; Multiple R-squared:  0.02832,    Adjusted R-squared:  0.02626 &lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="cp"&gt;##&lt;/span&gt;&lt;span class="c"&gt; F-statistic: 13.76 on 1 and 472 DF,  p-value: 0.0002328&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We plot the data points and fitted lines using different colors to distinguish between males (blue) and females (pink).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;plot&lt;span class="p"&gt;(&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Year&lt;span class="p"&gt;,&lt;/span&gt; hunger&lt;span class="o"&gt;$&lt;/span&gt;Numeric&lt;span class="p"&gt;,&lt;/span&gt; type &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;points&lt;span class="p"&gt;(&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Year&lt;span class="p"&gt;,&lt;/span&gt; hunger&lt;span class="o"&gt;$&lt;/span&gt;Numeric&lt;span class="p"&gt;,&lt;/span&gt; pch &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;19&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; col &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Sex &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Female&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;125&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;lines&lt;span class="p"&gt;(&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Year&lt;span class="p"&gt;[&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Sex &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Male&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; lmM&lt;span class="o"&gt;$&lt;/span&gt;fitted&lt;span class="p"&gt;,&lt;/span&gt; col &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;blue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; lwd &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;lines&lt;span class="p"&gt;(&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Year&lt;span class="p"&gt;[&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Sex &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Female&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; lmF&lt;span class="o"&gt;$&lt;/span&gt;fitted&lt;span class="p"&gt;,&lt;/span&gt; col &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;red&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; lwd &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="rimage center"&gt;&lt;img src="figure/multivariable-regression-4-1.png"/&gt;&lt;/div&gt;

&lt;p&gt;We can see from the plot that the lines are not exactly parallel. On the right side of the graph (around the year 2010) they are closer together than on the left side (around 1970). Slopes are -0.29340 for femals, -0.32340 for males.&lt;/p&gt;
&lt;p&gt;Now instead of separating the data by subsetting the samples by gender, we'll use gender as another predictor to create the linear model:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;lmBoth &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;Numeric &lt;span class="o"&gt;~&lt;/span&gt; Year &lt;span class="o"&gt;+&lt;/span&gt; Sex&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; hunger&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;lmBoth&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Call:&lt;/span&gt;
&lt;span class="code-line"&gt;## lm(formula = Numeric ~ Year + Sex, data = hunger)&lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Residuals:&lt;/span&gt;
&lt;span class="code-line"&gt;##     Min      1Q  Median      3Q     Max &lt;/span&gt;
&lt;span class="code-line"&gt;## -25.472 -11.297  -1.848   7.058  45.990 &lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Coefficients:&lt;/span&gt;
&lt;span class="code-line"&gt;##             Estimate Std. Error t value Pr(&amp;gt;|t|)    &lt;/span&gt;
&lt;span class="code-line"&gt;## (Intercept) 633.5283   120.8950   5.240 1.98e-07 ***&lt;/span&gt;
&lt;span class="code-line"&gt;## Year         -0.3084     0.0604  -5.106 3.99e-07 ***&lt;/span&gt;
&lt;span class="code-line"&gt;## SexMale       1.9027     0.8576   2.219   0.0267 *  &lt;/span&gt;
&lt;span class="code-line"&gt;## ---&lt;/span&gt;
&lt;span class="code-line"&gt;## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Residual standard error: 13.2 on 945 degrees of freedom&lt;/span&gt;
&lt;span class="code-line"&gt;## Multiple R-squared:  0.03175,    Adjusted R-squared:  0.0297 &lt;/span&gt;
&lt;span class="code-line"&gt;## F-statistic: 15.49 on 2 and 945 DF,  p-value: 2.392e-07&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that Male and Female are factors (factors are treated in alphabetic order, so reference, here, is the Female group).&lt;br /&gt;
The intercept represents the percentage of hungry females at year 0.&lt;br /&gt;
The estimate for the factor Male 1.9027 is a distance from the intercept (the estimate of the reference group Female). So the percentage of hungry males at year 0 is the sum of the intercept and the male estimate, ie. 633.5283 + 1.9027 = 635.431&lt;br /&gt;
The estimate for hunger$Year represents the annual decrease in percentage of both gender.&lt;/p&gt;
&lt;p&gt;In the plot, the red line will have the female intercept and the  blue line will have the male intercept:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;plot&lt;span class="p"&gt;(&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Year&lt;span class="p"&gt;,&lt;/span&gt; hunger&lt;span class="o"&gt;$&lt;/span&gt;Numeric&lt;span class="p"&gt;,&lt;/span&gt; pch &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;19&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;points&lt;span class="p"&gt;(&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Year&lt;span class="p"&gt;,&lt;/span&gt; hunger&lt;span class="o"&gt;$&lt;/span&gt;Numeric&lt;span class="p"&gt;,&lt;/span&gt; pch &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;19&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; col &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Sex &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Female&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;125&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;abline&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;lmBoth&lt;span class="o"&gt;$&lt;/span&gt;coeff&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; lmBoth&lt;span class="o"&gt;$&lt;/span&gt;coeff&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; col &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;red&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; lwd &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;abline&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;lmBoth&lt;span class="o"&gt;$&lt;/span&gt;coeff&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; lmBoth&lt;span class="o"&gt;$&lt;/span&gt;coeff&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; lmBoth&lt;span class="o"&gt;$&lt;/span&gt;coeff&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; col &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;blue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; lwd &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="rimage center"&gt;&lt;img src="figure/multivariable-regression-6-1.png"/&gt;&lt;/div&gt;

&lt;p&gt;The lines are parallels (since, they have the same slope lmBoth$coeff[2]).&lt;/p&gt;
&lt;p&gt;Now we'll consider the interaction between year and gender to see how that affects changes in rates of hunger:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;lmInter &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; lm&lt;span class="p"&gt;(&lt;/span&gt;Numeric &lt;span class="o"&gt;~&lt;/span&gt; Year &lt;span class="o"&gt;+&lt;/span&gt; Sex &lt;span class="o"&gt;+&lt;/span&gt; Sex &lt;span class="o"&gt;*&lt;/span&gt; Year&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; hunger&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;lmInter&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Call:&lt;/span&gt;
&lt;span class="code-line"&gt;## lm(formula = Numeric ~ Year + Sex + Sex * Year, data = hunger)&lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Residuals:&lt;/span&gt;
&lt;span class="code-line"&gt;##     Min      1Q  Median      3Q     Max &lt;/span&gt;
&lt;span class="code-line"&gt;## -25.913 -11.248  -1.853   7.087  46.146 &lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Coefficients:&lt;/span&gt;
&lt;span class="code-line"&gt;##               Estimate Std. Error t value Pr(&amp;gt;|t|)    &lt;/span&gt;
&lt;span class="code-line"&gt;## (Intercept)  603.50580  171.05519   3.528 0.000439 ***&lt;/span&gt;
&lt;span class="code-line"&gt;## Year          -0.29340    0.08547  -3.433 0.000623 ***&lt;/span&gt;
&lt;span class="code-line"&gt;## SexMale       61.94772  241.90858   0.256 0.797946    &lt;/span&gt;
&lt;span class="code-line"&gt;## Year:SexMale  -0.03000    0.12087  -0.248 0.804022    &lt;/span&gt;
&lt;span class="code-line"&gt;## ---&lt;/span&gt;
&lt;span class="code-line"&gt;## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/span&gt;
&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;## Residual standard error: 13.21 on 944 degrees of freedom&lt;/span&gt;
&lt;span class="code-line"&gt;## Multiple R-squared:  0.03181,    Adjusted R-squared:  0.02874 &lt;/span&gt;
&lt;span class="code-line"&gt;## F-statistic: 10.34 on 3 and 944 DF,  p-value: 1.064e-06&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The percentage of hungry females at year 0 is 603.5058.&lt;br /&gt;
The percentage of hungry males at year 0 is 665.4535 (603.50580  + 61.94772).&lt;br /&gt;
The annual change (decrease) in percentage of hungry females is 0.29340.&lt;br /&gt;
The estimate associated with Year:SexMale represents the distance of the annual change in percent of males from that of females.
The annual change in percentage of hungry males is -0.32340 (-0.29340 - 0.03000).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;plot&lt;span class="p"&gt;(&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Year&lt;span class="p"&gt;,&lt;/span&gt; hunger&lt;span class="o"&gt;$&lt;/span&gt;Numeric&lt;span class="p"&gt;,&lt;/span&gt; pch &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;19&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;points&lt;span class="p"&gt;(&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Year&lt;span class="p"&gt;,&lt;/span&gt; hunger&lt;span class="o"&gt;$&lt;/span&gt;Numeric&lt;span class="p"&gt;,&lt;/span&gt; pch &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;19&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; col &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;hunger&lt;span class="o"&gt;$&lt;/span&gt;Sex &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Male&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;125&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;abline&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;lmInter&lt;span class="o"&gt;$&lt;/span&gt;coeff&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; lmInter&lt;span class="o"&gt;$&lt;/span&gt;coeff&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; col &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;red&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; lwd &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;abline&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;lmInter&lt;span class="o"&gt;$&lt;/span&gt;coeff&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; lmInter&lt;span class="o"&gt;$&lt;/span&gt;coeff&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; lmInter&lt;span class="o"&gt;$&lt;/span&gt;coeff&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; lmInter&lt;span class="o"&gt;$&lt;/span&gt;coeff&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; col &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;blue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; lwd &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="rimage center"&gt;&lt;img src="figure/multivariable-regression-8-1.png"/&gt;&lt;/div&gt;

&lt;p&gt;The lines are not parallel and will eventually intersect. The Male blue line indicates a faster rate of change.&lt;/p&gt;
&lt;p&gt;Here we're dealing with an interaction between factors.&lt;/p&gt;
&lt;p&gt;Suppose we have two interacting predictors and one of them is held constant.&lt;br /&gt;
The expected change in the outcome for a unit change in the other predictor is the coefficient of that changing predictor  + the coefficient of the interaction * the value of the predictor held constant.&lt;/p&gt;
&lt;p&gt;For example, let's the model be : &lt;span class="math"&gt;\(H_i = b_0 + (b_1*I_i) + (b_2*Y_i)+ (b_3*I_i*Y_i)\)&lt;/span&gt; with H the outcomes, I's and Y's the predictors and b's the estimated coefficients of the predictors.&lt;br /&gt;
If I is fixed at a value, 5 and Y varies, &lt;span class="math"&gt;\(b_2 + b3*5\)&lt;/span&gt; represents the change in H per unit change in Y given that I is fixed at 5.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="R"></category></entry><entry><title>Kanban Practices</title><link href="http://stephanie-w.github.io/blog/kanban.html" rel="alternate"></link><updated>2015-07-02T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-07-02:blog/kanban.html</id><summary type="html">
&lt;hr/&gt;
&lt;!-- BEGIN_SUMMARY --&gt;
&lt;p&gt;Kanban in the context of software development mean a visual process-management system that tells what to produce, when to produce it, and how much to produce - inspired by the Toyota Production System and by Lean manufacturing.&lt;/p&gt;
&lt;!-- END_SUMMARY --&gt;
&lt;p&gt;A Kanban board must be visual, tactile, collaborative.  &lt;/p&gt;
&lt;h2 id="kanban-composants"&gt;Kanban Composants&lt;/h2&gt;
&lt;p&gt;The Kanban composants are :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kanban cards representing pieces of work. The cards can support various information as :&lt;ul&gt;
&lt;li&gt;assignee&lt;/li&gt;
&lt;li&gt;start time&lt;/li&gt;
&lt;li&gt;end time&lt;/li&gt;
&lt;li&gt;type of task (eg. bug vs feature vs tech debt (usually code enhancements))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Labeled containers for the cards representing the different stages of a workflow, the life-cycle of a task/story&lt;/li&gt;
&lt;li&gt;Constraints&lt;ul&gt;
&lt;li&gt;information on the exact product or component specifications that are needed for the subsequent process step, ie. to pull task/story from column to column&lt;/li&gt;
&lt;li&gt;max number of cards a column can contain&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:
A minimal Kanban board is a board with columns "To Do", "Doing", "Done":&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/kanban-board-1.png"/&gt;&lt;/p&gt;
&lt;p&gt;The most popular example of kanban board for agile or lean software development consists of: Backlog, Ready, Coding, Testing, Approval and Done columns. It is also a common practice to name columns in a different way, for example: Next, In Development, Done, Customer Acceptance, Live.&lt;/p&gt;
&lt;p&gt;An other common one is:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/kanban-board-2.png"/&gt;&lt;/p&gt;
&lt;p&gt;Possible workflow step could be chosen among the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Backlog&lt;/li&gt;
&lt;li&gt;Ready/Selected&lt;/li&gt;
&lt;li&gt;Coding/Development&lt;/li&gt;
&lt;li&gt;Testing&lt;/li&gt;
&lt;li&gt;Acceptance/Approval&lt;/li&gt;
&lt;li&gt;Deployment&lt;/li&gt;
&lt;li&gt;Done/Live&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="core-kanban-principles"&gt;Core Kanban Principles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Visualize the workflow, showing the work in progress, the work remaining and showing the flow of work through the Kanban system&lt;/li&gt;
&lt;li&gt;Limit the Work in Progress (WiP), and then reduce the time it takes an item to travel through the Kanban system&lt;/li&gt;
&lt;li&gt;Manage the Flow&lt;ul&gt;
&lt;li&gt;Visualize blocked items, long queues, empty spaces&lt;/li&gt;
&lt;li&gt;Deal with indicators of problems&lt;/li&gt;
&lt;li&gt;Analyze the flow with metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Implement Feedback Loops&lt;/li&gt;
&lt;li&gt;Make Process Policies Explicit&lt;/li&gt;
&lt;li&gt;Improve Collaboratively&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Kanban method starts with existing roles and process and stimulates continuous, incremental and evolutionary change in the system by monitoring, adapting and improving the workflow, by measuring effectiveness by tracking flow, quality, throughput, lead times.&lt;/p&gt;
&lt;h2 id="kanban-metrics"&gt;Kanban Metrics&lt;/h2&gt;
&lt;p&gt;First some definitions to understand Kanban Metrics.&lt;br/&gt;
Lead Time : The time it takes a work to get from step A to step B.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/Lead-Time.png"/&gt;&lt;/p&gt;
&lt;p&gt;There can be several lead times (e.g., customer lead time, development lead time, QA lead time, etc.) and specially the End to end Lead Time : The time in which a card goes from being created to being closed.&lt;br/&gt;
This metric say about how the whole organization or product team (not only a development team) reacts to customer's needs.&lt;/p&gt;
&lt;p&gt;Delivery/Cycle Lead Time : The time a card spend out of the backlog, ie. the elapsed time from the moment the team  starts actively working on a task till the moment they are done.&lt;br/&gt;
This metric basically say about how responsive the team is or how fast they can deliver something when priorities change.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/Cycle-Time.png"/&gt;&lt;/p&gt;
&lt;p&gt;Different teams will use different definitions for start and done ("accepted by the product owner" vs "delivered to production").&lt;/p&gt;
&lt;h3 id="cumulative-flow-diagrams"&gt;Cumulative Flow Diagrams&lt;/h3&gt;
&lt;p&gt;This report represents the relative amount of work for each stage of project over the time.
The key data points of the CFD are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The vertical distance between each area represents the amount of work in progress on the respective stage in a specific date&lt;/li&gt;
&lt;li&gt;The horizontal distance between the areas in the chart corresponds to the average lead time of the requests that arrived on a specific date&lt;/li&gt;
&lt;li&gt;The mean delivery rate, represented by the slope of the closed items area, corresponds to the trend in the delivery of the work.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This metric help understand the state of current work  and what might need to be done to speed up the pace of delivery.&lt;br/&gt;
The diagram should run smoothly. Large steps and flat horizontal lines indicate impediments to flow or lack of flow. Variations in the gap or bands stand for bottleneck situations, which usually occur due to irrelevant work in progress limits.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/CFD.PNG"/&gt;&lt;/p&gt;
&lt;h3 id="lead-time-average"&gt;Lead Time Average&lt;/h3&gt;
&lt;p&gt;This report shows a trend of the average number of days a task took to be completed, from start to finish. It is categorized by work item type so teams can see the how the system is performing over time, broken down by work item type. &lt;/p&gt;
&lt;h3 id="flow-efficiency"&gt;Flow Efficiency&lt;/h3&gt;
&lt;p&gt;This report shows the average percentage of lead time that a developer spent working on a task (touch time). This shows the potential for process improvements. &lt;/p&gt;
&lt;h3 id="cycle-time-diagram"&gt;Cycle time diagram&lt;/h3&gt;
&lt;p&gt;Stacked bar chart representing how much time a task has spent in a given state through is life-cycle, allowing the detection of tasks that are taking too long.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/cycle-time-column-2.png"/&gt;&lt;/p&gt;
&lt;h3 id="due-date-performance"&gt;Due Date Performance&lt;/h3&gt;
&lt;p&gt;This report shows the percentage of items that were delivered on time and the average percentage delivery rate over time. This is useful for illustrating how predictable the system is. Due date performance with a low percentage provides evidence that there is an abundance of variability in the flow. The team should take corrective action or they will not be able to establish reasonable service agreements. &lt;/p&gt;
&lt;h3 id="bugs-per-story"&gt;Bugs per Story&lt;/h3&gt;
&lt;p&gt;This report shows a trend of the average number of defects opened against a Story. Attaining predictability is a fundamental aspect in the Kanban system, however, that will be ineffective if the software is delivered with low quality. This chart is also useful when implementing Kanban to help teams establish goals in the quality area. &lt;/p&gt;
&lt;h3 id="lead-time-distribution"&gt;Lead Time Distribution&lt;/h3&gt;
&lt;p&gt;This report is a statistical distribution that shows the number of occurrences by lead time. It is an effective way to identify discrepancies in the process and boosts the confidence of teams in the definition of service level agreements, based on real life data. &lt;/p&gt;
&lt;h3 id="throughput-trend"&gt;Throughput Trend&lt;/h3&gt;
&lt;p&gt;This report shows the number of items that were delivered in a given time period (eg monthly). It can be used to identify a trend of how well the system has been performing. As teams work on process improvements, the throughput trend will show a more consistent slope in the chart. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/TroughputTrend.PNG"/&gt;&lt;/p&gt;
&lt;!--
##Kanban Strategies:
a kanban by project?
a kanban by team? a kanban by team seems more as one of the goal is to improve collaborative work.  
Team kanban can be combined with techniques like user map stories which allow to visualize project big pictures.

## Building a kanban

Methodologies to validate the model:

* Produce a sketch from your top-down or bottom-up model.
* Make sure that actual work items map to your sketch or top-down model, then use the "what does this item need?" questions.
* Consider whether it would be helpful to group, consolidate, or break down categories.


Links:

* Explain Kanban practices with a game [Kanban Pizza Game](http://www.agile42.com/en/training/kanban-pizza-game/)
* Kanban Board App [Trello](http://trello.com)
* Kanban Board App [Restyaboard](http://restya.com/board/)

### Six Rules for an Effective Kanban System

To ensure a proper setup of Kanban in the workplace, Toyota has provided us with six rules for an effective Kanban system:  
Customer (downstream) processes withdraw items in the precise amounts specified by the Kanban.  
Supplier (upstream) produces items in the precise amounts and sequences specified by the Kanban.  
No items are made or moved without a Kanban.  
A Kanban should accompany each item, every time.  
Defects and incorrect amounts are never sent to the next downstream process.  
The number of Kanbans is reduced carefully to lower inventories and to reveal problems.  
--&gt;
&lt;p&gt;Sources:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://positiveincline.com/index.php/2015/06/kanban-from-the-inside-20-model-workflow/"&gt;Kanban from the Inside: 20. Model workflow&lt;/a&gt;&lt;br/&gt;
&lt;a href="http://blog.crisp.se/2009/06/26/henrikkniberg/1246053060000"&gt;Kanban Story&lt;/a&gt;&lt;br/&gt;
&lt;a href="http://blog.assembla.com/AssemblaBlog/tabid/12618/bid/102123/Kanban-Metrics-Measure-Cycle-Time-To-Stay-Lean.aspx"&gt;Kanban Metrics: Measure Cycle Time To Stay Lean&lt;/a&gt;&lt;br/&gt;
&lt;a href="http://leankit.com/kanban/lean-flow-metrics/"&gt;7 LEAN METRICS TO IMPROVE FLOW&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://jazz.net/library/article/1350"&gt;Improve predictability and efficiency with Kanban metrics using IBM Rational Insight&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://www.atlassian.com/agile/kanban"&gt;A brief introduction to kanban&lt;/a&gt;
&lt;a href="http://brodzinski.com/2013/07/cumulative-flow-diagram.html"&gt;Cumulative Flow Diagram&lt;/a&gt;
&lt;a href="http://pm.stackexchange.com/questions/10657/what-are-some-commonly-tracked-metrics-in-kanban"&gt;Stack Exchange : What are commonly tracked metrics in kanban?&lt;/a&gt;
&lt;a href="http://blog.kanbanize.com/kanban-analytics-part-ii-cycle-time/"&gt;Kanban Analytics part II: Cycle Time&lt;/a&gt;
&lt;a href="http://agileramblings.com/tag/metrics-2/"&gt;Agile Rambling - Tag Archives: metrics&lt;/a&gt;&lt;/p&gt;</summary></entry><entry><title>R: Earthquakes from the past 30 days</title><link href="http://stephanie-w.github.io/blog/last_earthquakes.html" rel="alternate"></link><updated>2015-06-23T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-06-23:blog/last_earthquakes.html</id><summary type="html">&lt;hr /&gt;
&lt;p&gt;Earthquakes of the last 30 months from the earthquake.usgs.gov feed : http://earthquake.usgs.gov/earthquakes/feed/v1.0/csv.php feed. &lt;/p&gt;
&lt;p&gt;Below are the fields included in the spreadsheet output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;time&lt;/span&gt;
&lt;span class="code-line"&gt;latitude&lt;/span&gt;
&lt;span class="code-line"&gt;longitude&lt;/span&gt;
&lt;span class="code-line"&gt;depth&lt;/span&gt;
&lt;span class="code-line"&gt;mag&lt;/span&gt;
&lt;span class="code-line"&gt;magType&lt;/span&gt;
&lt;span class="code-line"&gt;nst&lt;/span&gt;
&lt;span class="code-line"&gt;gap&lt;/span&gt;
&lt;span class="code-line"&gt;dmin&lt;/span&gt;
&lt;span class="code-line"&gt;rms&lt;/span&gt;
&lt;span class="code-line"&gt;net&lt;/span&gt;
&lt;span class="code-line"&gt;id&lt;/span&gt;
&lt;span class="code-line"&gt;updated&lt;/span&gt;
&lt;span class="code-line"&gt;place&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Loading libraries and downloading the feed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;reshape2&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;ggplot2&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;ggmap&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;eq &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_month.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; as.is &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;eq&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;time&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;longitude&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;latitude&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;place&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;mag&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##                       time longitude latitude&lt;/span&gt;
&lt;span class="code-line"&gt;## 1 2015-08-20T22:41:58.790Z -117.1827 34.00583&lt;/span&gt;
&lt;span class="code-line"&gt;## 2 2015-08-20T22:09:37.180Z -121.5903 36.76000&lt;/span&gt;
&lt;span class="code-line"&gt;## 3 2015-08-20T21:38:51.403Z -119.6169 41.87000&lt;/span&gt;
&lt;span class="code-line"&gt;## 4 2015-08-20T21:27:08.570Z  -98.1158 36.69730&lt;/span&gt;
&lt;span class="code-line"&gt;## 5 2015-08-20T21:24:43.900Z  -64.5836 19.71500&lt;/span&gt;
&lt;span class="code-line"&gt;## 6 2015-08-20T21:17:33.170Z -116.4987 33.49567&lt;/span&gt;
&lt;span class="code-line"&gt;##                                          place  mag&lt;/span&gt;
&lt;span class="code-line"&gt;## 1                6km S of Redlands, California 0.92&lt;/span&gt;
&lt;span class="code-line"&gt;## 2             7km ESE of Prunedale, California 1.43&lt;/span&gt;
&lt;span class="code-line"&gt;## 3                 69km ESE of Lakeview, Oregon 2.48&lt;/span&gt;
&lt;span class="code-line"&gt;## 4                  21km NE of Helena, Oklahoma 2.70&lt;/span&gt;
&lt;span class="code-line"&gt;## 5 143km N of Road Town, British Virgin Islands 3.40&lt;/span&gt;
&lt;span class="code-line"&gt;## 6                 18km ESE of Anza, California 0.58&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Preprocessing the data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;eq&lt;span class="o"&gt;$&lt;/span&gt;area &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;^[^,]+, &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; eq&lt;span class="o"&gt;$&lt;/span&gt;place&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;eq&lt;span class="o"&gt;$&lt;/span&gt;date &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;strtrim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;eq&lt;span class="o"&gt;$&lt;/span&gt;time&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;19&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; format &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;%Y-%m-%dT%H:%M:%S&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Building a new dataset with magnitude frequencies by day:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;eqFreq1 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;with&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;eq&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; mag&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;eqFreq2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; melt&lt;span class="p"&gt;(&lt;/span&gt;eqFreq1&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;eqFreq2&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;magnitude&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;freq&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;subset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;eqFreq2&lt;span class="p"&gt;,&lt;/span&gt; freq &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; magnitude &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##            date magnitude freq&lt;/span&gt;
&lt;span class="code-line"&gt;## 1149 2015-07-22      0.01    1&lt;/span&gt;
&lt;span class="code-line"&gt;## 1150 2015-07-23      0.01    1&lt;/span&gt;
&lt;span class="code-line"&gt;## 1155 2015-07-28      0.01    1&lt;/span&gt;
&lt;span class="code-line"&gt;## 1156 2015-07-29      0.01    1&lt;/span&gt;
&lt;span class="code-line"&gt;## 1168 2015-08-10      0.01    1&lt;/span&gt;
&lt;span class="code-line"&gt;## 1173 2015-08-15      0.01    1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Plotting a stacked bars graph of earthquakes magnitude frequencies by day:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;eqFreq2&lt;span class="o"&gt;$&lt;/span&gt;magnitude &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;eqFreq2&lt;span class="o"&gt;$&lt;/span&gt;magnitude&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;ggplot&lt;span class="p"&gt;(&lt;/span&gt;eqFreq2&lt;span class="p"&gt;,&lt;/span&gt; aes&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; weight &lt;span class="o"&gt;=&lt;/span&gt; freq&lt;span class="p"&gt;,&lt;/span&gt; fill &lt;span class="o"&gt;=&lt;/span&gt; magnitude&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        geom_bar&lt;span class="p"&gt;(&lt;/span&gt;binwidth &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;60&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="m"&gt;60&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="m"&gt;24&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        labs&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; y &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Frequency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;             title &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Earthquakes Frequency from the past 30 days&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        theme&lt;span class="p"&gt;(&lt;/span&gt;axis.text.x &lt;span class="o"&gt;=&lt;/span&gt; element_text&lt;span class="p"&gt;(&lt;/span&gt;angle &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;90&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; hjust &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        scale_fill_brewer&lt;span class="p"&gt;(&lt;/span&gt;palette&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;BuPu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="rimage center"&gt;&lt;img src="figure/earthquakes_frequency-1.png" title="plot of chunk earthquakes_frequency" alt="plot of chunk earthquakes_frequency" class="plot" /&gt;&lt;/div&gt;

&lt;p&gt;Using the original dataset to plot a map with the locations of the 100 biggest earthquakes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;BiggestMag &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;tail&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;eq&lt;span class="o"&gt;$&lt;/span&gt;mag&lt;span class="p"&gt;),&lt;/span&gt; n&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="c1"&gt;#Plotting&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;world_map &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; map_data&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;world&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;ggplot&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; coord_fixed&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; xlab&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; ylab&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        geom_polygon&lt;span class="p"&gt;(&lt;/span&gt;data &lt;span class="o"&gt;=&lt;/span&gt; world_map&lt;span class="p"&gt;,&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                     aes&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; long&lt;span class="p"&gt;,&lt;/span&gt; y &lt;span class="o"&gt;=&lt;/span&gt; lat&lt;span class="p"&gt;,&lt;/span&gt; group &lt;span class="o"&gt;=&lt;/span&gt; group&lt;span class="p"&gt;),&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                     colour &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;light green&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; fill &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;light green&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        geom_point&lt;span class="p"&gt;(&lt;/span&gt;aes&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; longitude&lt;span class="p"&gt;,&lt;/span&gt; y &lt;span class="o"&gt;=&lt;/span&gt; latitude&lt;span class="p"&gt;,&lt;/span&gt; size &lt;span class="o"&gt;=&lt;/span&gt; mag&lt;span class="p"&gt;),&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;                   data &lt;span class="o"&gt;=&lt;/span&gt; eq&lt;span class="p"&gt;[&lt;/span&gt;eq&lt;span class="o"&gt;$&lt;/span&gt;mag &lt;span class="o"&gt;%in%&lt;/span&gt; BiggestMag &lt;span class="p"&gt;,],&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                   colour &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Deep Pink&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; fill &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Pink&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;                   pch &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;21&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; alpha &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        labs&lt;span class="p"&gt;(&lt;/span&gt;title&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Biggest earthquakes from the past 30 days&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="rimage center"&gt;&lt;img src="figure/earthquakes_worldmap-1.png" title="plot of chunk earthquakes_worldmap" alt="plot of chunk earthquakes_worldmap" class="plot" /&gt;&lt;/div&gt;</summary><category term="R"></category></entry><entry><title>Git Workflows vs "All to Master"</title><link href="http://stephanie-w.github.io/blog/git-workflows.html" rel="alternate"></link><updated>2015-06-20T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-06-20:blog/git-workflows.html</id><summary type="html">&lt;hr /&gt;
&lt;p&gt;A basic git workflow is the feature branch workflow. All feature (or task, or story) developments are taken place in a dedicated branch (the feature branch).&lt;br /&gt;
The stories/features branch exist as long as the feature is in development.&lt;br /&gt;
At the end of the development, the branch is usually submitted for review (pull request) and merged to master.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Feature Branch Flow" src="figure/gitflow-1.png" /&gt;&lt;/p&gt;
&lt;p&gt;Some layers can be added to this model:&lt;/p&gt;
&lt;p&gt;The integration/dev branch : It hold all feature branches commits, allow to check all pieces of dev work together.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Dev Branch Flow" src="figure/gitflow-2.png" /&gt;&lt;/p&gt;
&lt;p&gt;There could be variations on how and when to merge:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;merge to dev branch while the story is still in progress and merge dev branch to master when the story is complete &lt;/li&gt;
&lt;li&gt;merge the dev branch to master each time it pass the CI test successfully&lt;/li&gt;
&lt;li&gt;merge the dev branch to master when the product owner accept the user story implementation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The hotfix branch: It hold commits for severe bugs (ie. production bugs) and are merged to master and to the branches "above" (dev/integration and stories/features branches).  &lt;/p&gt;
&lt;p&gt;The release branch : It support preparation for a new production release. The dev/integration is merged to the release branch. Commits after a merge concern bugfixes of stories/features. These commits are also merged to the branches above.&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://www.atlassian.com/git/tutorials/comparing-workflows"&gt;Gitworklow Comparisons&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;Git workflows works well when interconnected code and infrastructure dependencies need to be managed and released in tandem.&lt;/p&gt;
&lt;p&gt;In Continuous deployment/delivery, the latest code is integrated and deployed automatically to any production environment.&lt;br /&gt;
This usually mean submitting all pull requests to master.&lt;br /&gt;
Each commit is a potential release candidate and is then subject to a series of test and verification steps to vheck it is ready to deploy.&lt;br /&gt;
The challenge in continuous deployment is determining what needs to be released together without manual intervention.&lt;/p&gt;
&lt;p&gt;Gitflow branching model vs "All to master":&lt;/p&gt;
&lt;p&gt;Gitflow pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;High degree of control of release content&lt;/li&gt;
&lt;li&gt;Handle projects with many layers of complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gitflow cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Complex rollback of individual feature once it is released&lt;/li&gt;
&lt;li&gt;Large merge into master making it difficult to visualize changing over time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All to master pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simplicity of individual feature rollback&lt;/li&gt;
&lt;li&gt;Immediate release of newly developed and approved features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All to master cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Master is a living entity on a software project and can be changed at any time; release tags are immutable.&lt;/li&gt;
&lt;li&gt;Difficult to coordinate simultaneous release of dependent code or infrastructure changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See &lt;a href="continuous-development.html"&gt;Continuous Development&lt;/a&gt; for more on continuous development. &lt;/p&gt;
&lt;p&gt;Sources : &lt;br /&gt;
&lt;a href="https://bocoup.com/weblog/git-workflows-for-successful-deployment/"&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://blogs.wandisco.com/2013/07/24/git-workflows-and-continuous-delivery-using-multisite-replication-to-facilitate-a-global-mainline/"&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://www.atlassian.com/git/tutorials/comparing-workflows"&gt;&lt;/a&gt;  &lt;/p&gt;</summary></entry><entry><title>R: Analysis of weather events impact on population health and economy.</title><link href="http://stephanie-w.github.io/blog/weather_event_impact.html" rel="alternate"></link><updated>2015-06-15T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-06-15:blog/weather_event_impact.html</id><summary type="html">&lt;hr /&gt;
&lt;h2 id="synopsis"&gt;Synopsis&lt;/h2&gt;
&lt;!-- BEGIN_SUMMARY --&gt;

&lt;p&gt;This analysis involves exploring the U.S. National Oceanic and Atmospheric Administration's (NOAA) storm database. &lt;/p&gt;
&lt;!-- END_SUMMARY --&gt;

&lt;p&gt;This database tracks characteristics of major storms and weather events in the United States, including when and where they occur, as well as estimates of any fatalities, injuries, and property damage.&lt;/p&gt;
&lt;p&gt;This analysis focuses on fatalities, injuries, property and crop damages to answer to the 2 following questions : 
Across the United States, which types of events (as indicated in the EVTYPE variable) are most harmful with respect to population health?
Across the United States, which types of events have the greatest economic consequences?&lt;/p&gt;
&lt;h2 id="data-processing"&gt;Data Processing&lt;/h2&gt;
&lt;p&gt;The data comes in the form of a comma-separated-value file compressed via the bzip2.&lt;/p&gt;
&lt;p&gt;We download it and unzip it in a ProjectData directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;download.file&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;FStormData.csv.bz2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    method &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;curl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; quiet &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;raw &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; read.csv&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;bzfile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;FStormData.csv.bz2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; sep &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; quote &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;\&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; header &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; stringsAsFactors &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;FALSE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;raw&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;BGN_DATE &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;raw&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;BGN_DATE&lt;span class="p"&gt;,&lt;/span&gt; format &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;%m/%d/%Y %H:%M:%S&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;raw&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;END_DATE &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;as.Date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;raw&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;END_DATE&lt;span class="p"&gt;,&lt;/span&gt; format &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;%m/%d/%Y %H:%M:%S&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Storm Events Database reports that all weather events are only recorded since 1996 (see http://www.ncdc.noaa.gov/stormevents/details.jsp).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;    Event Types Available:&lt;/span&gt;
&lt;span class="code-line"&gt;    Add more info about event types here. Link to collections page/tab when referencing data collection source.&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    1. Tornado: From 1950 through 1954, only tornado events were recorded.&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    2. Tornado, Thunderstorm Wind and Hail: From 1955 through 1992, only tornado, thunderstorm wind and hail events were keyed from the paper publications into digital data. From 1993 to 1995, only tornado, thunderstorm wind and hail events have been extracted from the Unformatted Text Files.&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    3. All Event Types (48 from Directive 10-1605): From 1996 to present, 48 event types are recorded as defined in NWS Directive 10-1605.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since we're reporting impacts from various weather events, we'll focus our study on data since 1996.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;data &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;raw&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;BGN_DATE &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;1996-01-01&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;rm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Weather events impact on population health across the United States since 1996&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To study the weather events impact on poulation health, we examine the INJURIES AND FATALITIES columns of the dataset.&lt;/p&gt;
&lt;p&gt;We compute the sum of total of injuries/fatalities by weather event type an store the values in a new dataset:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;fat_by_ev &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; aggregate&lt;span class="p"&gt;(&lt;/span&gt;FATALITIES &lt;span class="o"&gt;~&lt;/span&gt; EVTYPE&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; data&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; na.rm &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;inj_by_ev &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; aggregate&lt;span class="p"&gt;(&lt;/span&gt;INJURIES &lt;span class="o"&gt;~&lt;/span&gt; EVTYPE&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; data&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; na.rm &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For fatalities, the quantile function shows that only 1% of the values of the total of injuries/fatalities by event type are significant (&amp;gt; 326 fatalities, &amp;gt; 1635 injuries).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;quantile&lt;span class="p"&gt;(&lt;/span&gt;fat_by_ev&lt;span class="o"&gt;$&lt;/span&gt;FATALITIES&lt;span class="p"&gt;,&lt;/span&gt; probs &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##     90%     91%     92%     93%     94%     95%     96%     97%     98%     99%    100% &lt;/span&gt;
&lt;span class="code-line"&gt;##    4.00    5.74    9.88   14.02   28.80   57.90   78.08  105.32  198.92  326.14 1797.00&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;quantile&lt;span class="p"&gt;(&lt;/span&gt;inj_by_ev&lt;span class="o"&gt;$&lt;/span&gt;INJURIES&lt;span class="p"&gt;,&lt;/span&gt; probs &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##      90%      91%      92%      93%      94%      95%      96%      97%      98%      99%     100% &lt;/span&gt;
&lt;span class="code-line"&gt;##    21.60    25.74    39.76    70.04    85.76   171.90   304.56   477.80  1034.84  1635.64 20667.00&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We order the new datas by fatalities/injuries (decreasing order) and display the 10th first rows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;fat_temp &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; fat_by_ev&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;fat_by_ev&lt;span class="o"&gt;$&lt;/span&gt;FATALITIES&lt;span class="p"&gt;,&lt;/span&gt; decreasing &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;colnames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;fat_temp&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;event.type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;fatalities&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;inj_temp &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; inj_by_ev&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;inj_by_ev&lt;span class="o"&gt;$&lt;/span&gt;INJURIES&lt;span class="p"&gt;,&lt;/span&gt; decreasing &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;colnames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;inj_temp&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;event.type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;injuries&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;knitr&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;kable&lt;span class="p"&gt;(&lt;/span&gt;fat_temp&lt;span class="p"&gt;,&lt;/span&gt; caption &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total fatalities by weather event type since 1996&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;event.type&lt;/th&gt;
&lt;th align="right"&gt;fatalities&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;75&lt;/td&gt;
&lt;td align="left"&gt;EXCESSIVE HEAT&lt;/td&gt;
&lt;td align="right"&gt;1797&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;417&lt;/td&gt;
&lt;td align="left"&gt;TORNADO&lt;/td&gt;
&lt;td align="right"&gt;1511&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;96&lt;/td&gt;
&lt;td align="left"&gt;FLASH FLOOD&lt;/td&gt;
&lt;td align="right"&gt;887&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;213&lt;/td&gt;
&lt;td align="left"&gt;LIGHTNING&lt;/td&gt;
&lt;td align="right"&gt;650&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;99&lt;/td&gt;
&lt;td align="left"&gt;FLOOD&lt;/td&gt;
&lt;td align="right"&gt;414&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;286&lt;/td&gt;
&lt;td align="left"&gt;RIP CURRENT&lt;/td&gt;
&lt;td align="right"&gt;340&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;423&lt;/td&gt;
&lt;td align="left"&gt;TSTM WIND&lt;/td&gt;
&lt;td align="right"&gt;241&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;143&lt;/td&gt;
&lt;td align="left"&gt;HEAT&lt;/td&gt;
&lt;td align="right"&gt;237&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;161&lt;/td&gt;
&lt;td align="left"&gt;HIGH WIND&lt;/td&gt;
&lt;td align="right"&gt;235&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;16&lt;/td&gt;
&lt;td align="left"&gt;AVALANCHE&lt;/td&gt;
&lt;td align="right"&gt;223&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;kable&lt;span class="p"&gt;(&lt;/span&gt;inj_temp&lt;span class="p"&gt;,&lt;/span&gt; caption &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total injuries by weather event type since 1996&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;event.type&lt;/th&gt;
&lt;th align="right"&gt;injuries&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;417&lt;/td&gt;
&lt;td align="left"&gt;TORNADO&lt;/td&gt;
&lt;td align="right"&gt;20667&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;99&lt;/td&gt;
&lt;td align="left"&gt;FLOOD&lt;/td&gt;
&lt;td align="right"&gt;6758&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;75&lt;/td&gt;
&lt;td align="left"&gt;EXCESSIVE HEAT&lt;/td&gt;
&lt;td align="right"&gt;6391&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;213&lt;/td&gt;
&lt;td align="left"&gt;LIGHTNING&lt;/td&gt;
&lt;td align="right"&gt;4140&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;423&lt;/td&gt;
&lt;td align="left"&gt;TSTM WIND&lt;/td&gt;
&lt;td align="right"&gt;3629&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;96&lt;/td&gt;
&lt;td align="left"&gt;FLASH FLOOD&lt;/td&gt;
&lt;td align="right"&gt;1674&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;413&lt;/td&gt;
&lt;td align="left"&gt;THUNDERSTORM WIND&lt;/td&gt;
&lt;td align="right"&gt;1400&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;496&lt;/td&gt;
&lt;td align="left"&gt;WINTER STORM&lt;/td&gt;
&lt;td align="right"&gt;1292&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;167&lt;/td&gt;
&lt;td align="left"&gt;HURRICANE/TYPHOON&lt;/td&gt;
&lt;td align="right"&gt;1275&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;143&lt;/td&gt;
&lt;td align="left"&gt;HEAT&lt;/td&gt;
&lt;td align="right"&gt;1222&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Then plot the datas:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;ggplot2&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;ggplot&lt;span class="p"&gt;(&lt;/span&gt;fat_temp&lt;span class="p"&gt;,&lt;/span&gt; aes&lt;span class="p"&gt;(&lt;/span&gt;reorder&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;event.type&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;fatalities&lt;span class="p"&gt;),&lt;/span&gt; fatalities&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; geom_bar&lt;span class="p"&gt;(&lt;/span&gt;stat &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;identity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    labs&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Weather event type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; y &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Fatalities&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; title &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total fatalities by weather event type since 1996&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    theme&lt;span class="p"&gt;(&lt;/span&gt;axis.text.x &lt;span class="o"&gt;=&lt;/span&gt; element_text&lt;span class="p"&gt;(&lt;/span&gt;angle &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;90&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; hjust &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="rimage center"&gt;&lt;img src="figure/weather_events_impact_fatalities-1.png" title="plot of chunk weather_events_impact_fatalities" alt="plot of chunk weather_events_impact_fatalities" class="plot" /&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;ggplot&lt;span class="p"&gt;(&lt;/span&gt;inj_temp&lt;span class="p"&gt;,&lt;/span&gt; aes&lt;span class="p"&gt;(&lt;/span&gt;reorder&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;event.type&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;injuries&lt;span class="p"&gt;),&lt;/span&gt; injuries&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; geom_bar&lt;span class="p"&gt;(&lt;/span&gt;stat &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;identity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    labs&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Weather event type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; y &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Injuries&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; title &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total injuries by weather event type since 1996&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    theme&lt;span class="p"&gt;(&lt;/span&gt;axis.text.x &lt;span class="o"&gt;=&lt;/span&gt; element_text&lt;span class="p"&gt;(&lt;/span&gt;angle &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;90&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; hjust &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="rimage center"&gt;&lt;img src="figure/weather_events_impact_injuries-1.png" title="plot of chunk weather_events_impact_injuries" alt="plot of chunk weather_events_impact_injuries" class="plot" /&gt;&lt;/div&gt;

&lt;p&gt;To go deeper, we can examine single weather types by having a closer look on their frequency/injuries and fatalities ratio :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;plyr&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;fat_ev_count &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; count&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;FATALITIES &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt; vars &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;EVTYPE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;temp &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;fat_ev_count&lt;span class="p"&gt;,&lt;/span&gt; fat_by_ev&lt;span class="p"&gt;,&lt;/span&gt; by &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;EVTYPE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;ratio &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; temp&lt;span class="o"&gt;$&lt;/span&gt;FATALITIES&lt;span class="o"&gt;/&lt;/span&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;freq&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;colnames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;event.type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.frequency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.fatalities&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.fatalities.ratio&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;kable&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;event.frequency&lt;span class="p"&gt;,&lt;/span&gt; decreasing &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt; caption &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total fatalities by weather event type (ordered by frequency desc) since 1996&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;event.type&lt;/th&gt;
&lt;th align="right"&gt;event.frequency&lt;/th&gt;
&lt;th align="right"&gt;event.fatalities&lt;/th&gt;
&lt;th align="right"&gt;event.fatalities.ratio&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;63&lt;/td&gt;
&lt;td align="left"&gt;LIGHTNING&lt;/td&gt;
&lt;td align="right"&gt;608&lt;/td&gt;
&lt;td align="right"&gt;650&lt;/td&gt;
&lt;td align="right"&gt;1.069079&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;27&lt;/td&gt;
&lt;td align="left"&gt;FLASH FLOOD&lt;/td&gt;
&lt;td align="right"&gt;584&lt;/td&gt;
&lt;td align="right"&gt;887&lt;/td&gt;
&lt;td align="right"&gt;1.518836&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;20&lt;/td&gt;
&lt;td align="left"&gt;EXCESSIVE HEAT&lt;/td&gt;
&lt;td align="right"&gt;564&lt;/td&gt;
&lt;td align="right"&gt;1797&lt;/td&gt;
&lt;td align="right"&gt;3.186170&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;92&lt;/td&gt;
&lt;td align="left"&gt;TORNADO&lt;/td&gt;
&lt;td align="right"&gt;459&lt;/td&gt;
&lt;td align="right"&gt;1511&lt;/td&gt;
&lt;td align="right"&gt;3.291939&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;74&lt;/td&gt;
&lt;td align="left"&gt;RIP CURRENT&lt;/td&gt;
&lt;td align="right"&gt;301&lt;/td&gt;
&lt;td align="right"&gt;340&lt;/td&gt;
&lt;td align="right"&gt;1.129568&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;28&lt;/td&gt;
&lt;td align="left"&gt;FLOOD&lt;/td&gt;
&lt;td align="right"&gt;270&lt;/td&gt;
&lt;td align="right"&gt;414&lt;/td&gt;
&lt;td align="right"&gt;1.533333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;94&lt;/td&gt;
&lt;td align="left"&gt;TSTM WIND&lt;/td&gt;
&lt;td align="right"&gt;212&lt;/td&gt;
&lt;td align="right"&gt;241&lt;/td&gt;
&lt;td align="right"&gt;1.136793&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;75&lt;/td&gt;
&lt;td align="left"&gt;RIP CURRENTS&lt;/td&gt;
&lt;td align="right"&gt;179&lt;/td&gt;
&lt;td align="right"&gt;202&lt;/td&gt;
&lt;td align="right"&gt;1.128492&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;48&lt;/td&gt;
&lt;td align="left"&gt;HIGH WIND&lt;/td&gt;
&lt;td align="right"&gt;176&lt;/td&gt;
&lt;td align="right"&gt;235&lt;/td&gt;
&lt;td align="right"&gt;1.335227&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;1&lt;/td&gt;
&lt;td align="left"&gt;AVALANCHE&lt;/td&gt;
&lt;td align="right"&gt;173&lt;/td&gt;
&lt;td align="right"&gt;223&lt;/td&gt;
&lt;td align="right"&gt;1.289017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;38&lt;/td&gt;
&lt;td align="left"&gt;HEAT&lt;/td&gt;
&lt;td align="right"&gt;133&lt;/td&gt;
&lt;td align="right"&gt;237&lt;/td&gt;
&lt;td align="right"&gt;1.781955&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;104&lt;/td&gt;
&lt;td align="left"&gt;WINTER STORM&lt;/td&gt;
&lt;td align="right"&gt;118&lt;/td&gt;
&lt;td align="right"&gt;191&lt;/td&gt;
&lt;td align="right"&gt;1.618644&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;90&lt;/td&gt;
&lt;td align="left"&gt;THUNDERSTORM WIND&lt;/td&gt;
&lt;td align="right"&gt;107&lt;/td&gt;
&lt;td align="right"&gt;130&lt;/td&gt;
&lt;td align="right"&gt;1.214953&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;85&lt;/td&gt;
&lt;td align="left"&gt;STRONG WIND&lt;/td&gt;
&lt;td align="right"&gt;90&lt;/td&gt;
&lt;td align="right"&gt;103&lt;/td&gt;
&lt;td align="right"&gt;1.144444&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;22&lt;/td&gt;
&lt;td align="left"&gt;EXTREME COLD/WIND CHILL&lt;/td&gt;
&lt;td align="right"&gt;87&lt;/td&gt;
&lt;td align="right"&gt;125&lt;/td&gt;
&lt;td align="right"&gt;1.436782&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;21&lt;/td&gt;
&lt;td align="left"&gt;EXTREME COLD&lt;/td&gt;
&lt;td align="right"&gt;86&lt;/td&gt;
&lt;td align="right"&gt;113&lt;/td&gt;
&lt;td align="right"&gt;1.313953&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;41&lt;/td&gt;
&lt;td align="left"&gt;HEAVY SNOW&lt;/td&gt;
&lt;td align="right"&gt;75&lt;/td&gt;
&lt;td align="right"&gt;107&lt;/td&gt;
&lt;td align="right"&gt;1.426667&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;11&lt;/td&gt;
&lt;td align="left"&gt;COLD/WIND CHILL&lt;/td&gt;
&lt;td align="right"&gt;74&lt;/td&gt;
&lt;td align="right"&gt;95&lt;/td&gt;
&lt;td align="right"&gt;1.283784&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;45&lt;/td&gt;
&lt;td align="left"&gt;HIGH SURF&lt;/td&gt;
&lt;td align="right"&gt;63&lt;/td&gt;
&lt;td align="right"&gt;87&lt;/td&gt;
&lt;td align="right"&gt;1.380952&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;39&lt;/td&gt;
&lt;td align="left"&gt;HEAVY RAIN&lt;/td&gt;
&lt;td align="right"&gt;59&lt;/td&gt;
&lt;td align="right"&gt;94&lt;/td&gt;
&lt;td align="right"&gt;1.593220&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;58&lt;/td&gt;
&lt;td align="left"&gt;ICE STORM&lt;/td&gt;
&lt;td align="right"&gt;51&lt;/td&gt;
&lt;td align="right"&gt;82&lt;/td&gt;
&lt;td align="right"&gt;1.607843&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3&lt;/td&gt;
&lt;td align="left"&gt;BLIZZARD&lt;/td&gt;
&lt;td align="right"&gt;46&lt;/td&gt;
&lt;td align="right"&gt;70&lt;/td&gt;
&lt;td align="right"&gt;1.521739&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;29&lt;/td&gt;
&lt;td align="left"&gt;FOG&lt;/td&gt;
&lt;td align="right"&gt;35&lt;/td&gt;
&lt;td align="right"&gt;60&lt;/td&gt;
&lt;td align="right"&gt;1.714286&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;105&lt;/td&gt;
&lt;td align="left"&gt;WINTER WEATHER&lt;/td&gt;
&lt;td align="right"&gt;29&lt;/td&gt;
&lt;td align="right"&gt;33&lt;/td&gt;
&lt;td align="right"&gt;1.137931&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;101&lt;/td&gt;
&lt;td align="left"&gt;WILDFIRE&lt;/td&gt;
&lt;td align="right"&gt;28&lt;/td&gt;
&lt;td align="right"&gt;75&lt;/td&gt;
&lt;td align="right"&gt;2.678571&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;43&lt;/td&gt;
&lt;td align="left"&gt;HEAVY SURF/HIGH SURF&lt;/td&gt;
&lt;td align="right"&gt;27&lt;/td&gt;
&lt;td align="right"&gt;42&lt;/td&gt;
&lt;td align="right"&gt;1.555556&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;93&lt;/td&gt;
&lt;td align="left"&gt;TROPICAL STORM&lt;/td&gt;
&lt;td align="right"&gt;24&lt;/td&gt;
&lt;td align="right"&gt;57&lt;/td&gt;
&lt;td align="right"&gt;2.375000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;49&lt;/td&gt;
&lt;td align="left"&gt;HURRICANE&lt;/td&gt;
&lt;td align="right"&gt;23&lt;/td&gt;
&lt;td align="right"&gt;61&lt;/td&gt;
&lt;td align="right"&gt;2.652174&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;98&lt;/td&gt;
&lt;td align="left"&gt;URBAN/SML STREAM FLD&lt;/td&gt;
&lt;td align="right"&gt;22&lt;/td&gt;
&lt;td align="right"&gt;28&lt;/td&gt;
&lt;td align="right"&gt;1.272727&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;50&lt;/td&gt;
&lt;td align="left"&gt;HURRICANE/TYPHOON&lt;/td&gt;
&lt;td align="right"&gt;19&lt;/td&gt;
&lt;td align="right"&gt;64&lt;/td&gt;
&lt;td align="right"&gt;3.368421&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;inj_ev_count &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; count&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;INJURIES &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt; vars &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;EVTYPE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;temp &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;inj_ev_count&lt;span class="p"&gt;,&lt;/span&gt; inj_by_ev&lt;span class="p"&gt;,&lt;/span&gt; by &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;EVTYPE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;ratio &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; temp&lt;span class="o"&gt;$&lt;/span&gt;INJURIES&lt;span class="o"&gt;/&lt;/span&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;freq&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;colnames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;event.type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.frequency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.injuries&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.injuries.ratio&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;kable&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;event.frequency&lt;span class="p"&gt;,&lt;/span&gt; decreasing &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt; caption &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total Injuries by weather event type (ordered by frequency desc) since 1996&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;event.type&lt;/th&gt;
&lt;th align="right"&gt;event.frequency&lt;/th&gt;
&lt;th align="right"&gt;event.injuries&lt;/th&gt;
&lt;th align="right"&gt;event.injuries.ratio&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;56&lt;/td&gt;
&lt;td align="left"&gt;LIGHTNING&lt;/td&gt;
&lt;td align="right"&gt;2250&lt;/td&gt;
&lt;td align="right"&gt;4140&lt;/td&gt;
&lt;td align="right"&gt;1.840000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;85&lt;/td&gt;
&lt;td align="left"&gt;TORNADO&lt;/td&gt;
&lt;td align="right"&gt;1877&lt;/td&gt;
&lt;td align="right"&gt;20667&lt;/td&gt;
&lt;td align="right"&gt;11.010655&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;87&lt;/td&gt;
&lt;td align="left"&gt;TSTM WIND&lt;/td&gt;
&lt;td align="right"&gt;1492&lt;/td&gt;
&lt;td align="right"&gt;3629&lt;/td&gt;
&lt;td align="right"&gt;2.432306&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;83&lt;/td&gt;
&lt;td align="left"&gt;THUNDERSTORM WIND&lt;/td&gt;
&lt;td align="right"&gt;587&lt;/td&gt;
&lt;td align="right"&gt;1400&lt;/td&gt;
&lt;td align="right"&gt;2.385009&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;43&lt;/td&gt;
&lt;td align="left"&gt;HIGH WIND&lt;/td&gt;
&lt;td align="right"&gt;403&lt;/td&gt;
&lt;td align="right"&gt;1083&lt;/td&gt;
&lt;td align="right"&gt;2.687345&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;23&lt;/td&gt;
&lt;td align="left"&gt;FLASH FLOOD&lt;/td&gt;
&lt;td align="right"&gt;335&lt;/td&gt;
&lt;td align="right"&gt;1674&lt;/td&gt;
&lt;td align="right"&gt;4.997015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;99&lt;/td&gt;
&lt;td align="left"&gt;WILDFIRE&lt;/td&gt;
&lt;td align="right"&gt;184&lt;/td&gt;
&lt;td align="right"&gt;911&lt;/td&gt;
&lt;td align="right"&gt;4.951087&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;34&lt;/td&gt;
&lt;td align="left"&gt;HAIL&lt;/td&gt;
&lt;td align="right"&gt;165&lt;/td&gt;
&lt;td align="right"&gt;713&lt;/td&gt;
&lt;td align="right"&gt;4.321212&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;17&lt;/td&gt;
&lt;td align="left"&gt;EXCESSIVE HEAT&lt;/td&gt;
&lt;td align="right"&gt;162&lt;/td&gt;
&lt;td align="right"&gt;6391&lt;/td&gt;
&lt;td align="right"&gt;39.450617&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;79&lt;/td&gt;
&lt;td align="left"&gt;STRONG WIND&lt;/td&gt;
&lt;td align="right"&gt;147&lt;/td&gt;
&lt;td align="right"&gt;278&lt;/td&gt;
&lt;td align="right"&gt;1.891156&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;101&lt;/td&gt;
&lt;td align="left"&gt;WINTER STORM&lt;/td&gt;
&lt;td align="right"&gt;144&lt;/td&gt;
&lt;td align="right"&gt;1292&lt;/td&gt;
&lt;td align="right"&gt;8.972222&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;24&lt;/td&gt;
&lt;td align="left"&gt;FLOOD&lt;/td&gt;
&lt;td align="right"&gt;142&lt;/td&gt;
&lt;td align="right"&gt;6758&lt;/td&gt;
&lt;td align="right"&gt;47.591549&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;98&lt;/td&gt;
&lt;td align="left"&gt;WILD/FOREST FIRE&lt;/td&gt;
&lt;td align="right"&gt;130&lt;/td&gt;
&lt;td align="right"&gt;545&lt;/td&gt;
&lt;td align="right"&gt;4.192308&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;68&lt;/td&gt;
&lt;td align="left"&gt;RIP CURRENT&lt;/td&gt;
&lt;td align="right"&gt;107&lt;/td&gt;
&lt;td align="right"&gt;209&lt;/td&gt;
&lt;td align="right"&gt;1.953271&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;1&lt;/td&gt;
&lt;td align="left"&gt;AVALANCHE&lt;/td&gt;
&lt;td align="right"&gt;105&lt;/td&gt;
&lt;td align="right"&gt;156&lt;/td&gt;
&lt;td align="right"&gt;1.485714&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;38&lt;/td&gt;
&lt;td align="left"&gt;HEAVY SNOW&lt;/td&gt;
&lt;td align="right"&gt;102&lt;/td&gt;
&lt;td align="right"&gt;698&lt;/td&gt;
&lt;td align="right"&gt;6.843137&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;69&lt;/td&gt;
&lt;td align="left"&gt;RIP CURRENTS&lt;/td&gt;
&lt;td align="right"&gt;84&lt;/td&gt;
&lt;td align="right"&gt;294&lt;/td&gt;
&lt;td align="right"&gt;3.500000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;25&lt;/td&gt;
&lt;td align="left"&gt;FOG&lt;/td&gt;
&lt;td align="right"&gt;73&lt;/td&gt;
&lt;td align="right"&gt;712&lt;/td&gt;
&lt;td align="right"&gt;9.753425&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;37&lt;/td&gt;
&lt;td align="left"&gt;HEAVY RAIN&lt;/td&gt;
&lt;td align="right"&gt;73&lt;/td&gt;
&lt;td align="right"&gt;230&lt;/td&gt;
&lt;td align="right"&gt;3.150685&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;51&lt;/td&gt;
&lt;td align="left"&gt;ICE STORM&lt;/td&gt;
&lt;td align="right"&gt;53&lt;/td&gt;
&lt;td align="right"&gt;318&lt;/td&gt;
&lt;td align="right"&gt;6.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3&lt;/td&gt;
&lt;td align="left"&gt;BLIZZARD&lt;/td&gt;
&lt;td align="right"&gt;41&lt;/td&gt;
&lt;td align="right"&gt;385&lt;/td&gt;
&lt;td align="right"&gt;9.390244&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;15&lt;/td&gt;
&lt;td align="left"&gt;DUST STORM&lt;/td&gt;
&lt;td align="right"&gt;38&lt;/td&gt;
&lt;td align="right"&gt;376&lt;/td&gt;
&lt;td align="right"&gt;9.894737&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;36&lt;/td&gt;
&lt;td align="left"&gt;HEAT&lt;/td&gt;
&lt;td align="right"&gt;36&lt;/td&gt;
&lt;td align="right"&gt;1222&lt;/td&gt;
&lt;td align="right"&gt;33.944444&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;42&lt;/td&gt;
&lt;td align="left"&gt;HIGH SURF&lt;/td&gt;
&lt;td align="right"&gt;33&lt;/td&gt;
&lt;td align="right"&gt;146&lt;/td&gt;
&lt;td align="right"&gt;4.424242&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;102&lt;/td&gt;
&lt;td align="left"&gt;WINTER WEATHER&lt;/td&gt;
&lt;td align="right"&gt;30&lt;/td&gt;
&lt;td align="right"&gt;343&lt;/td&gt;
&lt;td align="right"&gt;11.433333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;90&lt;/td&gt;
&lt;td align="left"&gt;TSTM WIND/HAIL&lt;/td&gt;
&lt;td align="right"&gt;26&lt;/td&gt;
&lt;td align="right"&gt;95&lt;/td&gt;
&lt;td align="right"&gt;3.653846&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;95&lt;/td&gt;
&lt;td align="left"&gt;URBAN/SML STREAM FLD&lt;/td&gt;
&lt;td align="right"&gt;26&lt;/td&gt;
&lt;td align="right"&gt;79&lt;/td&gt;
&lt;td align="right"&gt;3.038461&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;100&lt;/td&gt;
&lt;td align="left"&gt;WIND&lt;/td&gt;
&lt;td align="right"&gt;26&lt;/td&gt;
&lt;td align="right"&gt;84&lt;/td&gt;
&lt;td align="right"&gt;3.230769&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;86&lt;/td&gt;
&lt;td align="left"&gt;TROPICAL STORM&lt;/td&gt;
&lt;td align="right"&gt;23&lt;/td&gt;
&lt;td align="right"&gt;338&lt;/td&gt;
&lt;td align="right"&gt;14.695652&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;11&lt;/td&gt;
&lt;td align="left"&gt;DENSE FOG&lt;/td&gt;
&lt;td align="right"&gt;20&lt;/td&gt;
&lt;td align="right"&gt;143&lt;/td&gt;
&lt;td align="right"&gt;7.150000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Weather events economic impact across Unites States&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We'll examine the property and crop damages columns (PROPDMG * 10^PROPDMGEXP and CROPDMG * 10^CROPDMGEXP, respectively) from the dataset.&lt;/p&gt;
&lt;p&gt;We need to preprocess the data to convert PROPDMG and PROPDMGEXP into numbers, idem for CROPDMG and CROPDMGEXP.&lt;/p&gt;
&lt;p&gt;Values for the CROPDMGEXP and PROPDMEXP are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0 or blank&lt;/li&gt;
&lt;li&gt;H for Hundred&lt;/li&gt;
&lt;li&gt;K for Thousand&lt;/li&gt;
&lt;li&gt;M for Million&lt;/li&gt;
&lt;li&gt;B for Billion&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: Except for these exponential values (H,K,M,B), there is some inexpected values : ? + - and numbers in the initial dataset. This values doesn't appear in the dataset filtered by BGN_DATE &amp;gt; 1996-01-01.&lt;/p&gt;
&lt;p&gt;Let's check the repartition of the exponential categories:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kp"&gt;table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;CROPDMGEXP&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;##             B      K      M &lt;/span&gt;
&lt;span class="code-line"&gt;## 373047      4 278685   1771&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kp"&gt;table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;PROPDMGEXP&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## &lt;/span&gt;
&lt;span class="code-line"&gt;##             0      B      K      M &lt;/span&gt;
&lt;span class="code-line"&gt;## 276166      1     32 369934   7374&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We convert PROPDMG/PROPDMGEXP and CROPDMG/CROPDMGEXP pairs into numbers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;data&lt;span class="o"&gt;$&lt;/span&gt;cropdmg.exp &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;CROPDMGEXP &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;H&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;cropdmg.exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;CROPDMGEXP &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;K&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;cropdmg.exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1000&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;CROPDMGEXP &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;M&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;cropdmg.exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;CROPDMGEXP &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;B&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;cropdmg.exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="m"&gt;9&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="o"&gt;$&lt;/span&gt;cropdmg.val &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; data&lt;span class="o"&gt;$&lt;/span&gt;CROPDMG &lt;span class="o"&gt;*&lt;/span&gt; data&lt;span class="o"&gt;$&lt;/span&gt;cropdmg.exp&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="o"&gt;$&lt;/span&gt;propdmg.exp &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;PROPDMGEXP &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;H&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;propdmg.exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;PROPDMGEXP &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;K&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;propdmg.exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1000&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;PROPDMGEXP &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;M&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;propdmg.exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1e+06&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;PROPDMGEXP &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;B&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;propdmg.exp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1e+09&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;data&lt;span class="o"&gt;$&lt;/span&gt;propdmg.val &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; data&lt;span class="o"&gt;$&lt;/span&gt;PROPDMG &lt;span class="o"&gt;*&lt;/span&gt; data&lt;span class="o"&gt;$&lt;/span&gt;propdmg.exp&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We perform the same analysis as for weather type impact on population health:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;crop_by_ev &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; aggregate&lt;span class="p"&gt;(&lt;/span&gt;cropdmg.val &lt;span class="o"&gt;~&lt;/span&gt; EVTYPE&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; data&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; na.rm &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;quantile&lt;span class="p"&gt;(&lt;/span&gt;crop_by_ev&lt;span class="o"&gt;$&lt;/span&gt;cropdmg.val&lt;span class="p"&gt;,&lt;/span&gt; probs &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##         90%         91%         92%         93%         94%         95%         96%         97% &lt;/span&gt;
&lt;span class="code-line"&gt;##       80000      522000     1830000     8518338    15105600    22067850    51986350   225801006 &lt;/span&gt;
&lt;span class="code-line"&gt;##         98%         99%        100% &lt;/span&gt;
&lt;span class="code-line"&gt;##   611260434  1328471682 13367566000&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;prop_by_ev &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; aggregate&lt;span class="p"&gt;(&lt;/span&gt;propdmg.val &lt;span class="o"&gt;~&lt;/span&gt; EVTYPE&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; data&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; na.rm &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;quantile&lt;span class="p"&gt;(&lt;/span&gt;prop_by_ev&lt;span class="o"&gt;$&lt;/span&gt;propdmg.val&lt;span class="p"&gt;,&lt;/span&gt; probs &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;seq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;##          90%          91%          92%          93%          94%          95%          96% &lt;/span&gt;
&lt;span class="code-line"&gt;##      6087080      7928162      9823200     19765792     40787880    117527100    551709366 &lt;/span&gt;
&lt;span class="code-line"&gt;##          97%          98%          99%         100% &lt;/span&gt;
&lt;span class="code-line"&gt;##   1328347705   4595502763  14205618003 143944833550&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;crop_temp &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; crop_by_ev&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;crop_by_ev&lt;span class="o"&gt;$&lt;/span&gt;cropdmg.val&lt;span class="p"&gt;,&lt;/span&gt; decreasing &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;colnames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;crop_temp&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;event.type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;cropdmg.val&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;prop_temp &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; prop_by_ev&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;prop_by_ev&lt;span class="o"&gt;$&lt;/span&gt;propdmg.val&lt;span class="p"&gt;,&lt;/span&gt; decreasing &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;colnames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;prop_temp&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;event.type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;propdmg.val&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;kable&lt;span class="p"&gt;(&lt;/span&gt;crop_temp&lt;span class="p"&gt;,&lt;/span&gt; caption &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total crop damages in (US dollars) by weather event type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;event.type&lt;/th&gt;
&lt;th align="right"&gt;cropdmg.val&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;59&lt;/td&gt;
&lt;td align="left"&gt;DROUGHT&lt;/td&gt;
&lt;td align="right"&gt;13367566000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;99&lt;/td&gt;
&lt;td align="left"&gt;FLOOD&lt;/td&gt;
&lt;td align="right"&gt;4974778400&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;166&lt;/td&gt;
&lt;td align="left"&gt;HURRICANE&lt;/td&gt;
&lt;td align="right"&gt;2741410000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;167&lt;/td&gt;
&lt;td align="left"&gt;HURRICANE/TYPHOON&lt;/td&gt;
&lt;td align="right"&gt;2607872800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;139&lt;/td&gt;
&lt;td align="left"&gt;HAIL&lt;/td&gt;
&lt;td align="right"&gt;2476029450&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;96&lt;/td&gt;
&lt;td align="left"&gt;FLASH FLOOD&lt;/td&gt;
&lt;td align="right"&gt;1334901700&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;81&lt;/td&gt;
&lt;td align="left"&gt;EXTREME COLD&lt;/td&gt;
&lt;td align="right"&gt;1288973000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;108&lt;/td&gt;
&lt;td align="left"&gt;FROST/FREEZE&lt;/td&gt;
&lt;td align="right"&gt;1094086000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;145&lt;/td&gt;
&lt;td align="left"&gt;HEAVY RAIN&lt;/td&gt;
&lt;td align="right"&gt;728169800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;420&lt;/td&gt;
&lt;td align="left"&gt;TROPICAL STORM&lt;/td&gt;
&lt;td align="right"&gt;677711000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;161&lt;/td&gt;
&lt;td align="left"&gt;HIGH WIND&lt;/td&gt;
&lt;td align="right"&gt;633561300&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;423&lt;/td&gt;
&lt;td align="left"&gt;TSTM WIND&lt;/td&gt;
&lt;td align="right"&gt;553915350&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;75&lt;/td&gt;
&lt;td align="left"&gt;EXCESSIVE HEAT&lt;/td&gt;
&lt;td align="right"&gt;492402000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;413&lt;/td&gt;
&lt;td align="left"&gt;THUNDERSTORM WIND&lt;/td&gt;
&lt;td align="right"&gt;398331000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;487&lt;/td&gt;
&lt;td align="left"&gt;WILDFIRE&lt;/td&gt;
&lt;td align="right"&gt;295472800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;417&lt;/td&gt;
&lt;td align="left"&gt;TORNADO&lt;/td&gt;
&lt;td align="right"&gt;283425010&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;102&lt;/td&gt;
&lt;td align="left"&gt;FREEZE&lt;/td&gt;
&lt;td align="right"&gt;146225000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;486&lt;/td&gt;
&lt;td align="left"&gt;WILD/FOREST FIRE&lt;/td&gt;
&lt;td align="right"&gt;106782330&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;150&lt;/td&gt;
&lt;td align="left"&gt;HEAVY SNOW&lt;/td&gt;
&lt;td align="right"&gt;71122100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;330&lt;/td&gt;
&lt;td align="left"&gt;STRONG WIND&lt;/td&gt;
&lt;td align="right"&gt;64953500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;kable&lt;span class="p"&gt;(&lt;/span&gt;prop_temp&lt;span class="p"&gt;,&lt;/span&gt; caption &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total property damages (in US dollars) by weather event type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;event.type&lt;/th&gt;
&lt;th align="right"&gt;propdmg.val&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;99&lt;/td&gt;
&lt;td align="left"&gt;FLOOD&lt;/td&gt;
&lt;td align="right"&gt;143944833550&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;167&lt;/td&gt;
&lt;td align="left"&gt;HURRICANE/TYPHOON&lt;/td&gt;
&lt;td align="right"&gt;69305840000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;327&lt;/td&gt;
&lt;td align="left"&gt;STORM SURGE&lt;/td&gt;
&lt;td align="right"&gt;43193536000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;417&lt;/td&gt;
&lt;td align="left"&gt;TORNADO&lt;/td&gt;
&lt;td align="right"&gt;24616905710&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;96&lt;/td&gt;
&lt;td align="left"&gt;FLASH FLOOD&lt;/td&gt;
&lt;td align="right"&gt;15222203910&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;139&lt;/td&gt;
&lt;td align="left"&gt;HAIL&lt;/td&gt;
&lt;td align="right"&gt;14595143420&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;166&lt;/td&gt;
&lt;td align="left"&gt;HURRICANE&lt;/td&gt;
&lt;td align="right"&gt;11812819010&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;420&lt;/td&gt;
&lt;td align="left"&gt;TROPICAL STORM&lt;/td&gt;
&lt;td align="right"&gt;7642475550&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;161&lt;/td&gt;
&lt;td align="left"&gt;HIGH WIND&lt;/td&gt;
&lt;td align="right"&gt;5247860360&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;487&lt;/td&gt;
&lt;td align="left"&gt;WILDFIRE&lt;/td&gt;
&lt;td align="right"&gt;4758667000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;328&lt;/td&gt;
&lt;td align="left"&gt;STORM SURGE/TIDE&lt;/td&gt;
&lt;td align="right"&gt;4641188000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;423&lt;/td&gt;
&lt;td align="left"&gt;TSTM WIND&lt;/td&gt;
&lt;td align="right"&gt;4478026440&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;191&lt;/td&gt;
&lt;td align="left"&gt;ICE STORM&lt;/td&gt;
&lt;td align="right"&gt;3642248810&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;413&lt;/td&gt;
&lt;td align="left"&gt;THUNDERSTORM WIND&lt;/td&gt;
&lt;td align="right"&gt;3382654440&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;486&lt;/td&gt;
&lt;td align="left"&gt;WILD/FOREST FIRE&lt;/td&gt;
&lt;td align="right"&gt;3001782500&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;496&lt;/td&gt;
&lt;td align="left"&gt;WINTER STORM&lt;/td&gt;
&lt;td align="right"&gt;1532733250&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;59&lt;/td&gt;
&lt;td align="left"&gt;DROUGHT&lt;/td&gt;
&lt;td align="right"&gt;1046101000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;213&lt;/td&gt;
&lt;td align="left"&gt;LIGHTNING&lt;/td&gt;
&lt;td align="right"&gt;743077080&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;150&lt;/td&gt;
&lt;td align="left"&gt;HEAVY SNOW&lt;/td&gt;
&lt;td align="right"&gt;634417540&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;437&lt;/td&gt;
&lt;td align="left"&gt;TYPHOON&lt;/td&gt;
&lt;td align="right"&gt;600230000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;ggplot&lt;span class="p"&gt;(&lt;/span&gt;crop_temp&lt;span class="p"&gt;,&lt;/span&gt; aes&lt;span class="p"&gt;(&lt;/span&gt;reorder&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;event.type&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;cropdmg.val&lt;span class="p"&gt;),&lt;/span&gt; cropdmg.val&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; geom_bar&lt;span class="p"&gt;(&lt;/span&gt;stat &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;identity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    labs&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Weather event type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; y &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Crop damages in US dollars&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; title &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total Crop damages by weather event type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    theme&lt;span class="p"&gt;(&lt;/span&gt;axis.text.x &lt;span class="o"&gt;=&lt;/span&gt; element_text&lt;span class="p"&gt;(&lt;/span&gt;angle &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;90&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; hjust &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="rimage center"&gt;&lt;img src="figure/weather_events_impact_crop_damages-1.png" title="plot of chunk weather_events_impact_crop_damages" alt="plot of chunk weather_events_impact_crop_damages" class="plot" /&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;ggplot&lt;span class="p"&gt;(&lt;/span&gt;prop_temp&lt;span class="p"&gt;,&lt;/span&gt; aes&lt;span class="p"&gt;(&lt;/span&gt;reorder&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;event.type&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;propdmg.val&lt;span class="p"&gt;),&lt;/span&gt; propdmg.val&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; geom_bar&lt;span class="p"&gt;(&lt;/span&gt;stat &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;identity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    labs&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Weather event type type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; y &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Properties damages in US dollars&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; title &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total Properties damages by weather event type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    theme&lt;span class="p"&gt;(&lt;/span&gt;axis.text.x &lt;span class="o"&gt;=&lt;/span&gt; element_text&lt;span class="p"&gt;(&lt;/span&gt;angle &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;90&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; hjust &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="rimage center"&gt;&lt;img src="figure/weather_events_impact_properties_damages-1.png" title="plot of chunk weather_events_impact_properties_damages" alt="plot of chunk weather_events_impact_properties_damages" class="plot" /&gt;&lt;/div&gt;

&lt;p&gt;and we check damages ratio by event type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;crop_ev_count &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; count&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;cropdmg.val &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt; vars &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;EVTYPE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;temp &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;crop_ev_count&lt;span class="p"&gt;,&lt;/span&gt; crop_by_ev&lt;span class="p"&gt;,&lt;/span&gt; by &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;EVTYPE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;ratio &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; temp&lt;span class="o"&gt;$&lt;/span&gt;cropdmg.val&lt;span class="o"&gt;/&lt;/span&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;freq&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;colnames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;event.type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.frequency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.cropdamages&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.cropdamages.ratio&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;kable&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;event.frequency&lt;span class="p"&gt;,&lt;/span&gt; decreasing &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt; caption &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total crop domages (in US dollars) by weather event type (ordered by frequency desc) since 1996&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;event.type&lt;/th&gt;
&lt;th align="right"&gt;event.frequency&lt;/th&gt;
&lt;th align="right"&gt;event.cropdamages&lt;/th&gt;
&lt;th align="right"&gt;event.cropdamages.ratio&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;22&lt;/td&gt;
&lt;td align="left"&gt;HAIL&lt;/td&gt;
&lt;td align="right"&gt;8100&lt;/td&gt;
&lt;td align="right"&gt;2476029450&lt;/td&gt;
&lt;td align="right"&gt;305682.65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;46&lt;/td&gt;
&lt;td align="left"&gt;TSTM WIND&lt;/td&gt;
&lt;td align="right"&gt;3440&lt;/td&gt;
&lt;td align="right"&gt;553915350&lt;/td&gt;
&lt;td align="right"&gt;161021.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;14&lt;/td&gt;
&lt;td align="left"&gt;FLASH FLOOD&lt;/td&gt;
&lt;td align="right"&gt;1845&lt;/td&gt;
&lt;td align="right"&gt;1334901700&lt;/td&gt;
&lt;td align="right"&gt;723523.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;15&lt;/td&gt;
&lt;td align="left"&gt;FLOOD&lt;/td&gt;
&lt;td align="right"&gt;1599&lt;/td&gt;
&lt;td align="right"&gt;4974778400&lt;/td&gt;
&lt;td align="right"&gt;3111180.99&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;44&lt;/td&gt;
&lt;td align="left"&gt;TORNADO&lt;/td&gt;
&lt;td align="right"&gt;1254&lt;/td&gt;
&lt;td align="right"&gt;283425010&lt;/td&gt;
&lt;td align="right"&gt;226016.75&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;43&lt;/td&gt;
&lt;td align="left"&gt;THUNDERSTORM WIND&lt;/td&gt;
&lt;td align="right"&gt;924&lt;/td&gt;
&lt;td align="right"&gt;398331000&lt;/td&gt;
&lt;td align="right"&gt;431094.16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;4&lt;/td&gt;
&lt;td align="left"&gt;DROUGHT&lt;/td&gt;
&lt;td align="right"&gt;243&lt;/td&gt;
&lt;td align="right"&gt;13367566000&lt;/td&gt;
&lt;td align="right"&gt;55010559.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;27&lt;/td&gt;
&lt;td align="left"&gt;HIGH WIND&lt;/td&gt;
&lt;td align="right"&gt;200&lt;/td&gt;
&lt;td align="right"&gt;633561300&lt;/td&gt;
&lt;td align="right"&gt;3167806.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;25&lt;/td&gt;
&lt;td align="left"&gt;HEAVY RAIN&lt;/td&gt;
&lt;td align="right"&gt;123&lt;/td&gt;
&lt;td align="right"&gt;728169800&lt;/td&gt;
&lt;td align="right"&gt;5920079.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;53&lt;/td&gt;
&lt;td align="left"&gt;URBAN/SML STREAM FLD&lt;/td&gt;
&lt;td align="right"&gt;112&lt;/td&gt;
&lt;td align="right"&gt;8488100&lt;/td&gt;
&lt;td align="right"&gt;75786.61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;17&lt;/td&gt;
&lt;td align="left"&gt;FROST/FREEZE&lt;/td&gt;
&lt;td align="right"&gt;102&lt;/td&gt;
&lt;td align="right"&gt;1094086000&lt;/td&gt;
&lt;td align="right"&gt;10726333.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;42&lt;/td&gt;
&lt;td align="left"&gt;STRONG WIND&lt;/td&gt;
&lt;td align="right"&gt;94&lt;/td&gt;
&lt;td align="right"&gt;64953500&lt;/td&gt;
&lt;td align="right"&gt;690994.68&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;47&lt;/td&gt;
&lt;td align="left"&gt;TSTM WIND/HAIL&lt;/td&gt;
&lt;td align="right"&gt;89&lt;/td&gt;
&lt;td align="right"&gt;64696250&lt;/td&gt;
&lt;td align="right"&gt;726924.16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;56&lt;/td&gt;
&lt;td align="left"&gt;WILDFIRE&lt;/td&gt;
&lt;td align="right"&gt;89&lt;/td&gt;
&lt;td align="right"&gt;295472800&lt;/td&gt;
&lt;td align="right"&gt;3319919.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;33&lt;/td&gt;
&lt;td align="left"&gt;LIGHTNING&lt;/td&gt;
&lt;td align="right"&gt;74&lt;/td&gt;
&lt;td align="right"&gt;6898440&lt;/td&gt;
&lt;td align="right"&gt;93222.16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;45&lt;/td&gt;
&lt;td align="left"&gt;TROPICAL STORM&lt;/td&gt;
&lt;td align="right"&gt;59&lt;/td&gt;
&lt;td align="right"&gt;677711000&lt;/td&gt;
&lt;td align="right"&gt;11486627.12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;28&lt;/td&gt;
&lt;td align="left"&gt;HURRICANE&lt;/td&gt;
&lt;td align="right"&gt;47&lt;/td&gt;
&lt;td align="right"&gt;2741410000&lt;/td&gt;
&lt;td align="right"&gt;58327872.34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;9&lt;/td&gt;
&lt;td align="left"&gt;EXTREME COLD&lt;/td&gt;
&lt;td align="right"&gt;46&lt;/td&gt;
&lt;td align="right"&gt;1288973000&lt;/td&gt;
&lt;td align="right"&gt;28021152.17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;55&lt;/td&gt;
&lt;td align="left"&gt;WILD/FOREST FIRE&lt;/td&gt;
&lt;td align="right"&gt;35&lt;/td&gt;
&lt;td align="right"&gt;106782330&lt;/td&gt;
&lt;td align="right"&gt;3050923.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;29&lt;/td&gt;
&lt;td align="left"&gt;HURRICANE/TYPHOON&lt;/td&gt;
&lt;td align="right"&gt;33&lt;/td&gt;
&lt;td align="right"&gt;2607872800&lt;/td&gt;
&lt;td align="right"&gt;79026448.48&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;prop_ev_count &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; count&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="p"&gt;[&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;propdmg.val &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt; vars &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;EVTYPE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;temp &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;prop_ev_count&lt;span class="p"&gt;,&lt;/span&gt; prop_by_ev&lt;span class="p"&gt;,&lt;/span&gt; by &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;EVTYPE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;ratio &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; temp&lt;span class="o"&gt;$&lt;/span&gt;propdmg.val&lt;span class="o"&gt;/&lt;/span&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;freq&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;colnames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kt"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;event.type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.frequency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.propdamages&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;event.propdamages.ratio&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;kable&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kp"&gt;order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;temp&lt;span class="o"&gt;$&lt;/span&gt;event.frequency&lt;span class="p"&gt;,&lt;/span&gt; decreasing &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt; caption &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Total property damages (in US dollars) by weather event type (ordered by frequency desc) since 1996&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;event.type&lt;/th&gt;
&lt;th align="right"&gt;event.frequency&lt;/th&gt;
&lt;th align="right"&gt;event.propdamages&lt;/th&gt;
&lt;th align="right"&gt;event.propdamages.ratio&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;142&lt;/td&gt;
&lt;td align="left"&gt;TSTM WIND&lt;/td&gt;
&lt;td align="right"&gt;60331&lt;/td&gt;
&lt;td align="right"&gt;4478026440&lt;/td&gt;
&lt;td align="right"&gt;74224.30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;137&lt;/td&gt;
&lt;td align="left"&gt;THUNDERSTORM WIND&lt;/td&gt;
&lt;td align="right"&gt;42726&lt;/td&gt;
&lt;td align="right"&gt;3382654440&lt;/td&gt;
&lt;td align="right"&gt;79170.87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;65&lt;/td&gt;
&lt;td align="left"&gt;HAIL&lt;/td&gt;
&lt;td align="right"&gt;20002&lt;/td&gt;
&lt;td align="right"&gt;14595143420&lt;/td&gt;
&lt;td align="right"&gt;729684.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;39&lt;/td&gt;
&lt;td align="left"&gt;FLASH FLOOD&lt;/td&gt;
&lt;td align="right"&gt;18647&lt;/td&gt;
&lt;td align="right"&gt;15222203910&lt;/td&gt;
&lt;td align="right"&gt;816335.28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;139&lt;/td&gt;
&lt;td align="left"&gt;TORNADO&lt;/td&gt;
&lt;td align="right"&gt;11847&lt;/td&gt;
&lt;td align="right"&gt;24616905710&lt;/td&gt;
&lt;td align="right"&gt;2077902.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;41&lt;/td&gt;
&lt;td align="left"&gt;FLOOD&lt;/td&gt;
&lt;td align="right"&gt;9086&lt;/td&gt;
&lt;td align="right"&gt;143944833550&lt;/td&gt;
&lt;td align="right"&gt;15842486.63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;96&lt;/td&gt;
&lt;td align="left"&gt;LIGHTNING&lt;/td&gt;
&lt;td align="right"&gt;8744&lt;/td&gt;
&lt;td align="right"&gt;743077080&lt;/td&gt;
&lt;td align="right"&gt;84981.37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;74&lt;/td&gt;
&lt;td align="left"&gt;HIGH WIND&lt;/td&gt;
&lt;td align="right"&gt;5217&lt;/td&gt;
&lt;td align="right"&gt;5247860360&lt;/td&gt;
&lt;td align="right"&gt;1005915.35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;131&lt;/td&gt;
&lt;td align="left"&gt;STRONG WIND&lt;/td&gt;
&lt;td align="right"&gt;3206&lt;/td&gt;
&lt;td align="right"&gt;174741450&lt;/td&gt;
&lt;td align="right"&gt;54504.51&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;166&lt;/td&gt;
&lt;td align="left"&gt;WINTER STORM&lt;/td&gt;
&lt;td align="right"&gt;1344&lt;/td&gt;
&lt;td align="right"&gt;1532733250&lt;/td&gt;
&lt;td align="right"&gt;1140426.53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;68&lt;/td&gt;
&lt;td align="left"&gt;HEAVY SNOW&lt;/td&gt;
&lt;td align="right"&gt;943&lt;/td&gt;
&lt;td align="right"&gt;634417540&lt;/td&gt;
&lt;td align="right"&gt;672765.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;67&lt;/td&gt;
&lt;td align="left"&gt;HEAVY RAIN&lt;/td&gt;
&lt;td align="right"&gt;906&lt;/td&gt;
&lt;td align="right"&gt;584864440&lt;/td&gt;
&lt;td align="right"&gt;645545.74&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;163&lt;/td&gt;
&lt;td align="left"&gt;WILDFIRE&lt;/td&gt;
&lt;td align="right"&gt;723&lt;/td&gt;
&lt;td align="right"&gt;4758667000&lt;/td&gt;
&lt;td align="right"&gt;6581835.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;157&lt;/td&gt;
&lt;td align="left"&gt;URBAN/SML STREAM FLD&lt;/td&gt;
&lt;td align="right"&gt;670&lt;/td&gt;
&lt;td align="right"&gt;58309650&lt;/td&gt;
&lt;td align="right"&gt;87029.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;84&lt;/td&gt;
&lt;td align="left"&gt;ICE STORM&lt;/td&gt;
&lt;td align="right"&gt;594&lt;/td&gt;
&lt;td align="right"&gt;3642248810&lt;/td&gt;
&lt;td align="right"&gt;6131732.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;141&lt;/td&gt;
&lt;td align="left"&gt;TROPICAL STORM&lt;/td&gt;
&lt;td align="right"&gt;390&lt;/td&gt;
&lt;td align="right"&gt;7642475550&lt;/td&gt;
&lt;td align="right"&gt;19596091.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;152&lt;/td&gt;
&lt;td align="left"&gt;TSTM WIND/HAIL&lt;/td&gt;
&lt;td align="right"&gt;380&lt;/td&gt;
&lt;td align="right"&gt;44320500&lt;/td&gt;
&lt;td align="right"&gt;116632.89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;167&lt;/td&gt;
&lt;td align="left"&gt;WINTER WEATHER&lt;/td&gt;
&lt;td align="right"&gt;373&lt;/td&gt;
&lt;td align="right"&gt;20866000&lt;/td&gt;
&lt;td align="right"&gt;55941.02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;162&lt;/td&gt;
&lt;td align="left"&gt;WILD/FOREST FIRE&lt;/td&gt;
&lt;td align="right"&gt;303&lt;/td&gt;
&lt;td align="right"&gt;3001782500&lt;/td&gt;
&lt;td align="right"&gt;9906872.94&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;88&lt;/td&gt;
&lt;td align="left"&gt;LAKE-EFFECT SNOW&lt;/td&gt;
&lt;td align="right"&gt;194&lt;/td&gt;
&lt;td align="right"&gt;40115000&lt;/td&gt;
&lt;td align="right"&gt;206778.35&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="conclusions"&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Wheater events impact on population health&lt;/em&gt;
Our analysys shows that since 1996:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The "Tornado" event caused the most injuries&lt;/li&gt;
&lt;li&gt;Both the "Tornado" and "Excessive heat" events caused the most fatalities&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From the frequency/ratio table, we can notice that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The "Lightning", "Flash Flood" and "Excessive Heat" are the most frequent events regarding events causing fatalities&lt;/li&gt;
&lt;li&gt;The "Lightning", "Tornado" and "TSM Wind" are the most frequent events regarding events causing injuries&lt;/li&gt;
&lt;li&gt;The "Excessive heat", "Flood" and "Heat", have a very high ratio regarding injuries&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Wheater events impact on economy&lt;/em&gt;
Our analysys shows that since 1996:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The "Drought" event caused the most crop damages&lt;/li&gt;
&lt;li&gt;The "Flood" events caused the most property damages&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From the frequency/ratio table, we can notice that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The "Hail" event is the most frequent event regarding events causing crop damages&lt;/li&gt;
&lt;li&gt;The "TSTM Wind" event is the most frequent event regarding events causing property damages&lt;/li&gt;
&lt;li&gt;The "Drought" and "Hurricane" events have a very high crop damages ratio&lt;/li&gt;
&lt;li&gt;The "Flood" and "Tropical storm" events have a very high property damages ratio&lt;/li&gt;
&lt;/ul&gt;</summary><category term="R"></category></entry><entry><title>The priority pyramid</title><link href="http://stephanie-w.github.io/blog/priority-pyramid.html" rel="alternate"></link><updated>2015-06-13T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-06-13:blog/priority-pyramid.html</id><summary type="html">&lt;hr /&gt;
&lt;p&gt;The priority pyramid is a visualized backlog with a prioritization process.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Priority Pyramid" src="figure/priority_pyramid.png" title="=600x500" /&gt;&lt;/p&gt;
&lt;p&gt;The pyramid has four level:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Priority One : The top of the pyramid which contains tasks/user stories with highest priority.&lt;/li&gt;
&lt;li&gt;Priority Two : The middle of the pyramid which contains tasks/user stories that will be started as soon as resources will be available.&lt;/li&gt;
&lt;li&gt;Priority Three : The bottom of the pyramid which contains tasks/user stories that will be worked on soon.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then the rest of the backlog below the pyramid.&lt;/p&gt;
&lt;p&gt;To each pyramid level is associated a maximum number of tasks/user stories a level can contain, the WIP-limit (Work-in-Process limitations).&lt;/p&gt;
&lt;p&gt;How to operate with the priority pyramid:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Priority Pyramid" src="figure/priority_pyramid_process.png" /&gt;&lt;/p&gt;
&lt;!--
Sources
http://jaxenter.com/agile-tips-the-priority-pyramid-116292.html
--&gt;</summary></entry><entry><title>Gantt Chart Principle</title><link href="http://stephanie-w.github.io/blog/gantt-chart-principle.html" rel="alternate"></link><updated>2015-06-12T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-06-12:blog/gantt-chart-principle.html</id><summary type="html">&lt;hr /&gt;
&lt;p&gt;A Gantt Chart illustrates a project schedule by presenting scheduling and dependencies information about the activities involved in implementing a project.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/gantt1.png" /&gt;&lt;/p&gt;
&lt;p&gt;The horizontal (x) axis of the chart shows the timescale over which the project work is carried out, and is divided into appropriate time units (the units of time used will depend on the overall timescale for the project). &lt;br /&gt;
The vertical (y) axis identifies the various tasks that must be carried out.&lt;br /&gt;
Each task is represented by a horizontal bar, occupying its own row on the y-axis.&lt;br /&gt;
The left-hand end of the bar is positioned on the x-axis in such a way that it represents the start date for the task, while the right hand end of the bar represents the finish date. The length of the bar therefore represents the duration of the task by definition.&lt;br /&gt;
Task dependencies may also be indicated using an arrow that links the end of one task to the beginning of the next. &lt;/p&gt;
&lt;p&gt;To illustrate the principle of a Gantt Chart building, let's take the opening of a store abroad as project.&lt;br /&gt;
The first step is listing the tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose a country&lt;/li&gt;
&lt;li&gt;Recruit a manager&lt;/li&gt;
&lt;li&gt;Find a site&lt;/li&gt;
&lt;li&gt;Purchase the site&lt;/li&gt;
&lt;li&gt;Get a permit&lt;/li&gt;
&lt;li&gt;Recruit the staff&lt;/li&gt;
&lt;li&gt;Modify the building&lt;/li&gt;
&lt;li&gt;Buy furniture&lt;/li&gt;
&lt;li&gt;Install furniture&lt;/li&gt;
&lt;li&gt;Open&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next step is drawing a network diagram (or the activity-on-arrow diagram or PERT Chart) to organize tasks according to dependencies. To each task is assigned a duration estimation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="PERT Chart" src="figure/pertchart.png" /&gt;&lt;/p&gt;
&lt;p&gt;The longest path of planned activities to logical end points or to the end of the project is in red. The sequence of project network activities which add up to the longest overall duration. This determines the shortest time possible to complete the project.&lt;/p&gt;
&lt;p&gt;We use this path to begin the Gantt Chart.&lt;br /&gt;
Then we position the other task according to the task they depend on.  &lt;/p&gt;
&lt;p&gt;The "find a site" is a floating task depending on "choose a country" task. The "find a site" task must occur before the "purchase a site" task.&lt;br /&gt;
The "manager" and the buy a furniture are sharing float tasks, coming after "choose a country" task and before "install furniture" task.&lt;br /&gt;
The best obvious idea in the project is to set the "recruit a manager" task as early as possible in this large floating space since it can help on other tasks and the "buy furniture" quite late before the installation.&lt;br /&gt;
The "recruit staff" task comes after the "recruit a manager" task and before the "open" task. Since it has a cost, the better idea is to set the "staff" task as late as possible. The "recruit staff" is a floating task depending on an other floating task. If the "recruit a manager" task takes longer or starts later, there will be less float space for the "recruit a staff" task. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Gantt Chart" src="figure/gantt-chart.png" /&gt;&lt;/p&gt;
&lt;!--
Sources:
http://www.oracle.com/webfolder/ux/middleware/richclient/index.html?/webfolder/ux/middleware/richclient/guidelines5/gantt.html
--&gt;</summary></entry><entry><title>R: Experimentation of the Cental Limit Theorem</title><link href="http://stephanie-w.github.io/blog/experimentation-of-clt.html" rel="alternate"></link><updated>2015-06-11T00:00:00+02:00</updated><author><name>"Stephanie W"</name></author><id>tag:stephanie-w.github.io,2015-06-11:blog/experimentation-of-clt.html</id><summary type="html">&lt;hr /&gt;
&lt;p&gt;&lt;!-- BEGIN_SUMMARY --&gt;
&lt;!-- END_SUMMARY --&gt;&lt;/p&gt;
&lt;p&gt;The CLT Theorem: &lt;br /&gt;
The distribution of sample statistics (e.g. mean) is approximatively normal, regardless of the underlying distribution, with mean = &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and variance = &lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To experiment this theory, we've built a matrix with 40 exponentials (we take lambda=0.2) x 1000 and compute the mean of each row, storing the result in a &lt;code&gt;means&lt;/code&gt; vector:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kp"&gt;set.seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;n &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;40&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;nosim &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1000&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;lambda &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;0.2&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;means &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kp"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;rexp&lt;span class="p"&gt;(&lt;/span&gt;nosim &lt;span class="o"&gt;*&lt;/span&gt; n&lt;span class="p"&gt;,&lt;/span&gt; lambda&lt;span class="p"&gt;),&lt;/span&gt; nosim&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kp"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kp"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;means&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;## [1] 4.602510 6.017790 5.463686 4.176755 7.144672 4.427567&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and plot an histogram of the &lt;code&gt;means&lt;/code&gt; vector and the density distribution:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;ggplot2&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;dat &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kt"&gt;data.frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;nosim&lt;span class="p"&gt;,&lt;/span&gt; y &lt;span class="o"&gt;=&lt;/span&gt; means&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;ggplot&lt;span class="p"&gt;(&lt;/span&gt;dat&lt;span class="p"&gt;,&lt;/span&gt; aes&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; y&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; geom_histogram&lt;span class="p"&gt;(&lt;/span&gt;colour &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;black&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; fill &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    geom_vline&lt;span class="p"&gt;(&lt;/span&gt;aes&lt;span class="p"&gt;(&lt;/span&gt;xintercept &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;y&lt;span class="p"&gt;)),&lt;/span&gt; color &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;red&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; linetype &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;dashed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;        size &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; labs&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Means&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; y &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Frequency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; title &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Means Frequency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="rimage center"&gt;&lt;img src="figure/means_histogram-1.png" title="plot of chunk means_histogram" alt="plot of chunk means_histogram" class="plot" /&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;ggplot&lt;span class="p"&gt;(&lt;/span&gt;dat&lt;span class="p"&gt;,&lt;/span&gt; aes&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; y&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; geom_histogram&lt;span class="p"&gt;(&lt;/span&gt;binwidth &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; colour &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;black&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; fill &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    aes&lt;span class="p"&gt;(&lt;/span&gt;y &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;..&lt;/span&gt;density..&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; stat_function&lt;span class="p"&gt;(&lt;/span&gt;fun &lt;span class="o"&gt;=&lt;/span&gt; dnorm&lt;span class="p"&gt;,&lt;/span&gt; args &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kt"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;mean &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;lambda&lt;span class="p"&gt;,&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    sd &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;lambda&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="kp"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;n&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; labs&lt;span class="p"&gt;(&lt;/span&gt;x &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Means&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; y &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Dendity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; title &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Distribution&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;/span&gt;
&lt;span class="code-line"&gt;    geom_density&lt;span class="p"&gt;(&lt;/span&gt;alpha &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; fill &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;#FF6666&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="rimage center"&gt;&lt;img src="figure/means_distribution-1.png" title="plot of chunk means_distribution" alt="plot of chunk means_distribution" class="plot" /&gt;&lt;/div&gt;

&lt;p&gt;To confirm this distribution fits the normal distribution, we've draw the hump of the density of a random variable normally distributed with a mean &lt;span class="math"&gt;\(1/\lambda\)&lt;/span&gt; and a standard deviation &lt;span class="math"&gt;\(\frac{1/\lambda}{\sqrt{n}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="R"></category><category term="stats"></category></entry><entry><title>Hadoop: Word Count Code explained</title><link href="http://stephanie-w.github.io/blog/hadoop-word-count-explained.html" rel="alternate"></link><updated>2015-06-08T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-06-08:blog/hadoop-word-count-explained.html</id><summary type="html">&lt;hr /&gt;
&lt;!-- BEGIN_SUMMARY --&gt;

&lt;!-- END_SUMMARY --&gt;

&lt;p&gt;Map Reduce Data Flow Diagram&lt;/p&gt;
&lt;p&gt;&lt;img alt="Map Reduce WordCount Diagram" src="figure/wordcount.png" /&gt;&lt;/p&gt;
&lt;h2 id="mapper-code"&gt;Mapper Code&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;java.io.IOException&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;java.util.StringTokenizer&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.io.IntWritable&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.io.LongWritable&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.io.Text&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.MapReduceBase&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.Mapper&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.OutputCollector&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.Reporter&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;WordCountMapper&lt;/span&gt; &lt;span class="n"&gt;extends&lt;/span&gt; &lt;span class="n"&gt;MapReduceBase&lt;/span&gt; &lt;span class="n"&gt;implements&lt;/span&gt; &lt;span class="n"&gt;Mapper&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;LongWritable&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IntWritable&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The mapper class (WordCountMapper) is static and extends MapReduceBase and implements Mapper.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;    &lt;span class="kd"&gt;private&lt;/span&gt; &lt;span class="kd"&gt;final&lt;/span&gt; &lt;span class="n"&gt;IntWritable&lt;/span&gt; &lt;span class="n"&gt;one&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;IntWritable&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="kd"&gt;private&lt;/span&gt; &lt;span class="n"&gt;Text&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The variable &lt;code&gt;one&lt;/code&gt; of IntWritable type is initialized to one. The variables &lt;code&gt;one&lt;/code&gt; and &lt;code&gt;word&lt;/code&gt; are the key/value pair respectively.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;map&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LongWritable&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Text&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;OutputCollector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IntWritable&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Reporter&lt;/span&gt; &lt;span class="n"&gt;reporter&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The map method takes as parameters a LongWritable, which is the key, a text, which is the value and a OutputCollector which takes a Text and IntWritable output, then the reporter for status reporting.
It throws a file exception if the file can't be acceeses for some reason.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;        &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;StringTokenizer&lt;/span&gt; &lt;span class="n"&gt;itr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;StringTokenizer&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toLowerCase&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The SringTokenizer class is a utility class in the MapReduceAPI that breaks down line into words. That's the defaut method fo tokenize lines. But Many times, in production context, this class will be extended or replaced.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;        &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;itr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;hasMoreTokens&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;            &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;set&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;itr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;nextToken&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;            &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;collect&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="o"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="o"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="o"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;While the tokenizer has still tokens, take the word and set the tokenizer to the next token. Then collect &lt;code&gt;word&lt;/code&gt; and &lt;code&gt;one&lt;/code&gt; in the output collector.&lt;/p&gt;
&lt;p&gt;This code is runned on each of the physical nodes and takes chunks of text from the HDFS file system by default and break them into a set of key/value pairs (ie. word and one)&lt;/p&gt;
&lt;h2 id="reducer-code"&gt;Reducer Code&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;java.io.IOException&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;java.util.Iterator&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.io.IntWritable&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.io.Text&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.io.WritableComparable&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.MapReduceBase&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.OutputCollector&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.Reducer&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.Reporter&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;WordCountReducer&lt;/span&gt; &lt;span class="kd"&gt;extends&lt;/span&gt; &lt;span class="n"&gt;MapReduceBase&lt;/span&gt; &lt;span class="kd"&gt;implements&lt;/span&gt; &lt;span class="n"&gt;Reducer&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IntWritable&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IntWritable&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Reduce class &lt;code&gt;WordCountReducer&lt;/code&gt; extends MapReduceBase and implements a reducer.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;reduce&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Iterator&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;IntWritable&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;OutputCollector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IntWritable&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Reporter&lt;/span&gt; &lt;span class="n"&gt;reporter&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;hasNext&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;            &lt;span class="c1"&gt;// replace ValueType with the real type of your value&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;            &lt;span class="n"&gt;IntWritable&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;IntWritable&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;next&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;            &lt;span class="n"&gt;sum&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;get&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt; &lt;span class="c1"&gt;// process value&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="o"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;collect&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;IntWritable&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="o"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="o"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The reducer aggregates or counts up the number of word of each type. Then collect the key and sum in the output collector, producing smaller summary of word count.&lt;/p&gt;
&lt;p&gt;All the values with the same key are presented to a single reducer together.
The reducer receives a key and an iterator of input values from an input list, returning a single output value.&lt;/p&gt;
&lt;p&gt;To resume:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;mapper(filename, file-contents):&lt;/span&gt;
&lt;span class="code-line"&gt;  for each word in file-contents:&lt;/span&gt;
&lt;span class="code-line"&gt;    emit (word, 1)&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;reducer(word values):&lt;/span&gt;
&lt;span class="code-line"&gt;  sum = 0&lt;/span&gt;
&lt;span class="code-line"&gt;  for each value in values:&lt;/span&gt;
&lt;span class="code-line"&gt;sum = sum + value&lt;/span&gt;
&lt;span class="code-line"&gt;emit(word, sum)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2 id="driver-code"&gt;Driver Code&lt;/h2&gt;
&lt;p&gt;The mapreduce code is runned by a main method, called the Driver.
The driver initializes the job and instructs the Hadoop platform to execute the code on a set of input files, and controls where the output files are placed.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.fs.Path&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.io.IntWritable&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.io.Text&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.FileInputFormat&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.FileOutputFormat&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.JobClient&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.mapred.JobConf&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;WordCount&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create an instance of job configuration that parse the WordCount.class:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;        &lt;span class="n"&gt;JobClient&lt;/span&gt; &lt;span class="n"&gt;client&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;JobClient&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;JobConf&lt;/span&gt; &lt;span class="n"&gt;conf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;JobConf&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;WordCount&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Specify output types:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;        &lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;setOutputKeyClass&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;setOutputValueClass&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;IntWritable&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Specify input and output directories:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;        &lt;span class="n"&gt;FileInputFormat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;addInputPath&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;input&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;  &lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;FileOutputFormat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;setOutputPath&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;output&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Specify a mapper:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;        &lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;setMapperClass&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;WordCountMapper&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Specify a reducer:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;        &lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;setReducerClass&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;WordCountReducer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;setCombinerClass&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;WordCountReducer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;setConf&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run the Job by submitting the job to Mapreduce:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;        &lt;span class="k"&gt;try&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;            &lt;span class="n"&gt;JobClient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;runJob&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="k"&gt;catch&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;            &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;printStackTrace&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;        &lt;span class="o"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;    &lt;span class="o"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span class="code-line"&gt;&lt;span class="o"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2 id="high-level-mapreduce-pipeline"&gt;High level MapReduce Pipeline&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="https://farm4.static.flickr.com/3126/3529146657_5b5d025a5f_o.png" /&gt;&lt;/p&gt;
&lt;p&gt;In details:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://farm3.static.flickr.com/2275/3529146683_c8247ff6db_o.png" /&gt;&lt;/p&gt;
&lt;p&gt;The read and split up of input files are defined by the InputFormat which performs the following tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Selects the files or other objects that should be used for input&lt;/li&gt;
&lt;li&gt;Defines the InputSplits that break a file into tasks&lt;/li&gt;
&lt;li&gt;Provides a factory for RecordReader objects that read the file&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Standard InputFormat are :&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;InputFormat&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Key&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;TextInputFormat&lt;/td&gt;
&lt;td&gt;Default format;&lt;br&gt;reads lines of text files&lt;/td&gt;
&lt;td&gt;The byte offset of the line&lt;/td&gt;
&lt;td&gt;The line contents&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;KeyValueInputFormat&lt;/td&gt;
&lt;td&gt;Parses lines into key,&lt;br&gt; val pair&lt;/td&gt;
&lt;td&gt;Everything up to the first tab character&lt;/td&gt;
&lt;td&gt;The remainder of the line&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SequenceFileInputFormat&lt;/td&gt;
&lt;td&gt;A Hadoop-specific high-performance binary format&lt;/td&gt;
&lt;td&gt;user-defined&lt;/td&gt;
&lt;td&gt;user-defined&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The InputSplit describes a unit of work that comprises a single map task in a MapReduce program.&lt;br /&gt;
By processing a file in chunks, several map tasks can operate on a single file in parallel. Since the various blocks that make up the file may be spread across several different nodes in the cluster, tasks can be scheduled on each of these different node.&lt;br /&gt;
The RecordReader class loads the data from its source and converts it into (key, value) pairs suitable for reading by the Mapper. The key associated with each line is its byte offset in the file. The RecordReader is invoke repeatedly on the input until the entire InputSplit has been consumed.&lt;/p&gt;
&lt;p&gt;After the mapping process, the (intermediate) outputs are moved to the reducers (shuffling). A different subset of the intermediate key space is assigned to each reduce node; these subsets (known as "partitions") are the inputs to the reduce tasks. The map nodes must all agree on where to send the different pieces of the intermediate data. The Partitioner class determines which partition a given (key, value) pair will go to.&lt;/p&gt;
&lt;p&gt;Each reduce task is responsible for reducing the values associated with several intermediate keys. The set of intermediate keys on a single node is automatically sorted by Hadoop before they are presented to the Reducer.&lt;/p&gt;
&lt;p&gt;After the reducing process the (key, value) pairs provided to this OutputCollector are then written to output files.&lt;br /&gt;
iThe instances of OutputFormat provided by Hadoop write to files on the local disk or in HDFS; they all inherit from a common FileOutputFormat. Each Reducer writes a separate file in a common output directory (usually named part-nnn).  &lt;/p&gt;
&lt;p&gt;Standard OutputFormat are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;OutputFormat&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;TextOutputFormat&lt;/td&gt;
&lt;td&gt;Default; writes lines in "key \t value" form&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SequenceFileOutputFormat&lt;/td&gt;
&lt;td&gt;Writes binary files suitable for reading into subsequent MapReduce jobs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NullOutputFormat&lt;/td&gt;
&lt;td&gt;Disregards its inputs&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;RecordWriters are used to write the individual records to the files as directed by the OutputFormat.&lt;/p&gt;
&lt;!--
Sources:
developpper.yahoo.com/hadoop/tutorial/module4.html
--&gt;</summary><category term="hadoop"></category></entry><entry><title>User stories</title><link href="http://stephanie-w.github.io/blog/user-stories.html" rel="alternate"></link><updated>2015-06-02T00:00:00+02:00</updated><author><name>Stephanie W</name></author><id>tag:stephanie-w.github.io,2015-06-02:blog/user-stories.html</id><summary type="html">&lt;hr /&gt;
&lt;!-- BEGIN_SUMMARY --&gt;

&lt;p&gt;A user story is a tool used in Agile software development to capture a description of a software feature from an end-user perspective. The user story describes the type of user, what they want and why. A user story helps to create a simplified description of a requirement.&lt;/p&gt;
&lt;!-- END_SUMMARY --&gt;

&lt;p&gt;A user story typically follow a simple template:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="code-line"&gt;As a &amp;lt;type of user&amp;gt;, I want &amp;lt;some goal&amp;gt; so that &amp;lt;some reason&amp;gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;User stories can be written on cards or sticky notes, stores on walls or tables to facilitate planning and discussion.&lt;/p&gt;
&lt;p&gt;They can be written at varying levels of detail from a high-level strategic viewpoint (typically at Feasibility) through to a more detailed, implementable level (typically during Exploration and Engineering). The large user stories are generally known as epics.
The epic is usually split into smaller user stories independent from one another so the team can worked on.&lt;/p&gt;
&lt;h2 id="persona-and-user-needs"&gt;Persona and user needs&lt;/h2&gt;
&lt;p&gt;Since the user story involves a type of user, even before writing the first user story, the product team had to work on a profile for typical product users. Such a profile looks like a short biography ; it explains the motivations guiding the user's actions and it's called a Persona. 
During the same product definition phase, the product team listed the needs of each of the personas. They selected the needs to be addressed by the product, and left the other needs aside.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Personas" src="figure/from-idea-to-product-14-638.jpg" /&gt;&lt;/p&gt;
&lt;h2 id="priorization"&gt;Priorization&lt;/h2&gt;
&lt;p&gt;They can be estimated/prioritized with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sequence numbers (simple, Fibonacci)&lt;/li&gt;
&lt;li&gt;T-shirt sizes (e.g. S, M, L, XL)&lt;/li&gt;
&lt;li&gt;Traffic lights (green, yellow, red)&lt;/li&gt;
&lt;li&gt;Starbucks drink sizes (demi, short, tall, grande, venti, trenta)&lt;/li&gt;
&lt;li&gt;A Moscow categorization : MoSCoW stands for:&lt;ul&gt;
&lt;li&gt;Must have (or Minimum Usable Subset)&lt;/li&gt;
&lt;li&gt;Should have (not critical but important)&lt;/li&gt;
&lt;li&gt;Could have (wanted or desirable but less important)&lt;/li&gt;
&lt;li&gt;Won’t have this time (explicitly excluded from scope for the planned duration but Would like in future)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="using-user-stories-to-build-a-roadmap"&gt;Using user stories to build a roadmap&lt;/h2&gt;
&lt;h3 id="story-maps"&gt;Story Maps&lt;/h3&gt;
&lt;p&gt;User story mapping is a technique allowing to see the big picture of the backlog and helping making decisions about grooming ans prioritizing the backlog, managing scopes.&lt;br /&gt;
See &lt;a href="http://winnipegagilist.blogspot.fr/2012/03/how-to-create-user-story-map.html"&gt;How to create a User Story Map&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="figure/story-map.png" /&gt;&lt;/p&gt;
&lt;p&gt;The high level user stories (blue post-its) are the epics. They can be split into smaller user stories which forms the "walking skeleton" of the map.&lt;br /&gt;
The user stories (pink post-its) are organized under each epic in order of highest to lowest priority for that feature.&lt;br /&gt;
The chronological order of how users will typically use the application goes left to right (Time).  &lt;/p&gt;
&lt;h3 id="product-tree"&gt;Product Tree&lt;/h3&gt;
&lt;p&gt;The product tree is a game board representing a tree organizing features to help teams to collaboratively develop roadmaps and strategic plans with customers.&lt;br /&gt;
The product tree is build during a "Prune the Product Tree" game where players draw limbs and locate post-its (user stories) on them.  &lt;br /&gt;
Leaves represents user stories.&lt;br /&gt;
The roots represents the requirements (dev team, infra, etc.).&lt;br /&gt;
Thick limbs represents major areas of functionality (or epics).Thin limbs can be drawn from thick limbs as guides to be more specificic (themes).&lt;/p&gt;
&lt;p&gt;The goal of the product tree is to build a well balanced tree by "pruning the tree" of unnecessary limbs or at the opposite adding new (missing) leaves.&lt;/p&gt;
&lt;h3 id="other-techniques-of-visualizations"&gt;Other techniques of visualizations&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.mountaingoatsoftware.com/blog/visualizing-a-large-product-backlog-with-a-treemap"&gt;Visualizing a Large Product Backlog With a Treemap&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://www.uxbooth.com/articles/stuck-in-the-details-mind-map-user-tasks/"&gt;Stuck in the Details? Mind Map User Tasks&lt;/a&gt;
&lt;!--
Sources :
http://www.agilemodeling.com/artifacts/userStory.htm
http://winnipegagilist.blogspot.fr/2012/03/how-to-create-user-story-map.html
http://fr.slideshare.net/mikecohn/prioritizing-your-product-backlog-22870228?related=1
http://www.mountaingoatsoftware.com/blog/visualizing-a-large-product-backlog-with-a-treemap
http://www.innovationgames.com/prune-the-product-tree/
http://www.scrumdesk.com/lets-grow-your-tree-of-requirements/
http://jpattonassociates.com/the-new-backlog/
--&gt;&lt;/p&gt;</summary></entry></feed>